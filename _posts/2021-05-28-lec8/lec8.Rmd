---
title: "Boostrap, Bagging, and Random Forest"
description: lec8
author:
  - name: hanbyeol
    url: {}
date: 04-16-2021
output:
  distill::distill_article:
    html_document:
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    smooth_scroll: true
    number_sections: true
    theme: cosmo
    highlight: tango
    code_folding: code
    editor_options: 
      markdown: 
    markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
library(tidyverse)
```

# 1. Boostrap

boostrap: 가지고 있는 data를 고정시키고, sample을 복원추출로 다시 뽑는다

x: 원래 내가 가지고 있는 data

```{r}
x <- rnorm(10, mean=10, sd=3)
x
mean(x)
```

가지고 있는 data의 갯수만큼 sample을 복원추출로 뽑는다

```{r}
sample(x, length(x), replace = TRUE)
```


그때의 복원추출 된 sample들의 평균을 본다 

*replace = TRUE* : 뽑을때마다 매번 달라짐

그때의 복원추출 된 sample들의 평균을 본다 


```{r}
mean(sample(x, length(x), replace = TRUE))
```
**어떻게 더 좋은 추정값을 구할 수 있을까??**

--> 복원추출로 구한 평균 값들을 다 평균내서 모평균을 추정하면 더 좋을것 같다.


1000개를 기준으로 mean값을 구해서 추정해보기


# 2. Bagging

for문으로 돌리기 

x: 한번뽑은 x는 변하지 않아야해서 for문 밖으로

```{r}
result <-rep(0,1000)
x <- rnorm(10, mean=10, sd=3)

for (i in 1:1000){

  result[i] <- mean(sample(x, length(x), replace = TRUE))
}

mean(result)
```

위의 for문이 반복될수록 더욱 안정적인 추정값이 된다. 
-> test set에 대한 변동성을 안정적으로 만들기 위해서는 가상의 data set을 만들어서 학습시키고 평균을 내면 된다. (*Bagging*)


행을 기준으로 sample을 뽑아야한다. 가로 한 줄이 표본 하나가 되는 것(가로 열을 기준으로 복원추출을 하는 것)

추정의 변동성이 작아진다-> train정보를 이용해서 boost 적용해서

가상의 data set을 만들면 test set이 바뀌어도 예측값의 변동성이 훨씬 더 안정적이다.


mtcars를 사용해서 예를들어보자

```{r}
mtcars
dim(mtcars)
```

가로 열을 기준으로 복원추출

mtcars에서 mpg를 예측하는데 3개의 변수로 회귀분석해서 예측했었음!

이젠 *Bagging*을 하기 위해서 가상의 data set을 만들어야한다. 


**같은 크기의 가상의 data set 5개 만들기**

```{r}
sample(mtcars, length(mtcars), replace = TRUE)
```

mtcars는 행을 기준으로 1개의 sample(하나의 표본)이니까, 가로를 기준으로 복원추출해야한다.
한번 선택한 표본은 그 자체가 들어간다.

```{r}
mtcars[c(1,3),]

index <- sample(1:nrow(mtcars), nrow(mtcars), replace = TRUE)
data_A <- mtcars[index,]

index <- sample(1:nrow(mtcars), nrow(mtcars), replace = TRUE)
data_B <- mtcars[index,]

index <- sample(1:nrow(mtcars), nrow(mtcars), replace = TRUE)
data_C <- mtcars[index,]

index <- sample(1:nrow(mtcars), nrow(mtcars), replace = TRUE)
data_D <- mtcars[index,]

index <- sample(1:nrow(mtcars), nrow(mtcars), replace = TRUE)
data_E <- mtcars[index,]
```

1:32(nrow)까지 벡터 중에서 32개(nrow)만큼 매번 새로 뽑을 것이다 

이 구조를 좀 더 간단하게 바꾸면

*my_f()*: 돌릴 때마다 다른 결과값을 가져옴


*sapply*: 벡터로 결과를 보여줌 -> my_f에서 구한 여러 값들을 합칠 수 있다


```{r}
my_f <- function(){
  index <- sample(nrow(mtcars), nrow(mtcars), replace=TRUE)
  mtcars[index,]
}

my_f()
# result <- sapply(1:5, my_f) # 에러남
```


행 말고 열도 랜덤으로 뽑아서 쓸수 있다(복원추출은 아님)

모델을 학습 시킬 떄 다 쓰는게 아니라 랜덤으로 뽑아서 쓸 수 있다.

장점? 오버피팅이 없음! 각 fitting된 애들의 학습 모델들이 train의 정보를 학습 할 때 예측값들이 서로 독립적으로 표현됨

이 과정들이 *bagging*이다. 

여기서 한단계 더 나아간 것이 *Random Forest*


# 3. Random forest

mtcars data에서 행 만을 복원추출로 뽑았었는데 random forest는 행 뿐만아니라 열도 랜덤으로 선택해서 쓸 수 있다. (복원추출은 아님)


예를들어 32:11 크기의 data에서 11개를 다 쓰는게 아니라 그 중에서 5개를 랜덤으로 뽑아서 쓴다 -> 모델을 학습시킬 때 변수의 일정부분만 랜덤으로 쓰는것

### Random Forest의 장점
  - partial information만 사용하기 때문에 over fitting이 방지된다.
  - 모델 하나하나의 예측 성능들이 독립적이다.

실생활의 예로 설명해보면 면접자를 뽑을 때 여러 평가들이 있는데 각 평가마다의 심사위원들이 다르다.

각 심사위원들이 여러 변수로 평가를 한 다음 과반수 이상이 찬성하면 뽑는것

classification tree를 만들어서 학습할 때: 모델의 깊이, 특정 기준으로 split할 때 어떤 것을 기준으로 split해야하는지 모두 고려해서 tree를 만들었다. 


random forest에서는 변수들을 다 고려하지 않고 몇개를 선택해서 최고인 것들을 선택한다음 여러개의 tree를 만든다. 

*ensemble*기법: 굳이 tree를 가지고 하지 않고, 다른 적용할 수 있는 모델들에서 표본을 복원추출 한다음 boostrapping하고, 변수를 random으로 선택해서 학습하고, aggregate하면 ensemble기법이 되는 것

--> 근데 왜 random forest를 쓰는가? 복잡한 모델을 가지고 시간도 줄고 성능도 좋아지는 것을 확인했기 때문에! tree가 ensemble하기 좋은 모델임