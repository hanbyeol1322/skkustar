---
title: "신용카드 사용자 연체 예측 AI 경진대회 using stacking"
description: 지구온나나나팀
author:
  - name: hanbyeol
    url: {}
date: 05-28-2021
output:
  distill::distill_article:
    html_document:
    fig_caption: true
    toc: true
    toc_float: yes
    fig_width: 5
    fig_height: 4
    smooth_scroll: true
    number_sections: true
    theme: cosmo
    highlight: tango
    editor_options: 
      markdown: 
    markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Dacon에서 진행하는 신용카드 사용자 연체 예측 AI 경진대회에서 지구온나나나팀은 mlr, randomforest, 그리고 deep learning을 사용해서 stacking 했습니다!

저는 그 중 randomforest 부분을 맡았습니다~

# 1. Data

### 1) Library

```{r}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(kknn)
library(stacks)
library(glmnet)
library(xgboost)
library(kernlab)
library(keras)
library(ranger)
library(purrr) ; library(magrittr) ; library(MASS)
```

keras를 돌리기 위해서 설치필요, 아나콘다 & 텐서플로우 install.packages("tensorflow") require(tensorflow) install.packages("curl") install_tensorflow()



### 2) 파일경로

```{r}
file_path <- "./" 
files <- list.files(file_path) 
files

```

### 3) 파일 불러와서 이름정리

```{r}
train <- read_csv("train.csv") %>% 
  janitor::clean_names()
test <- read_csv( "test.csv") %>%
  janitor::clean_names()
```

### 4) train data의 문자형(character data)를 요인(factor)으로 바꿔주기 & 예측하고자 하는 credit을 요인(factor)으로 설정하기

```{r}
train %<>%
  mutate_if(is.character, as.factor) %>% 
  mutate(credit = factor(credit))
```

### 5) test data의 문자형 데이터를 factor로 바꿔주기

```{r}

test %<>%
  mutate_if(is.character, as.factor)

```

### 6) 전처리

  - *recipe(credit ~ .)*: credit을 y로 하는 모형으로(credit외의 변수를 예측하는데 쓸것임)

  - *yrs_birth*: 나이
  
  - *yrs_employed*: 근무기간
  
  - *perincome*: 인당소득
  
  - *adult_income*: 성인소득
  
  - *begin_month*: 신용카드를 발급한지 몇달 됐니
  
  - *step_rm*: 데이터 제거
  
  - *step_unknown*: 결측값에 "unknown" factor 할당
  
  - *step_zv*:분산이 0인 변수를 제거
  
  - *step_integer*: nominal data(순서 없이 분류된, eg 독어,영어,일어)에 각각 유닉한 정수를 부여
  
  - *step_center*: 평균을 0으로 만들기
  
  - *prep*: training으로 'train'data를 사용한다
  
```{r}

credit_recipe <- train %>% 
  recipe(credit ~ .) %>%
  step_mutate(yrs_birth = -ceiling(days_birth/365), 
              yrs_employed = -ceiling(days_employed/365), 
              perincome = income_total / family_size, 
              adult_income = (family_size - child_num) * income_total, 
              begin_month = -begin_month) %>% 
  step_rm(index, days_birth, work_phone, phone, email) %>%  
  step_unknown(occyp_type) %>% 
  step_zv(all_predictors()) %>% 
  step_integer(all_nominal(), -all_outcomes()) %>% 
  step_center(all_predictors(), -all_outcomes()) %>% 
  prep(training = train)
```

### 7) 전처리한 데이터 준비하기

  - *bake*: 전처리한 데이터를 예측모형에 사용할건데, new_data를 test로 사용해라
  
```{r}
train2 <- juice(credit_recipe)
test2 <- bake(credit_recipe, new_data= test)

```

### 8) 튜닝준비하기(cross validation 이용)

  - *vfold_cv*: train2 data를 5-fold로 나누고 그룹을 credit을 기준으로 균등하게(strata) 뽑을것임
  
```{r}
set.seed(2021)

validation_split <- vfold_cv(train2, v = 5, strata = credit)
```

# 2. Random Forest

### 1) Random Forest 모수추정

random forest 사용할 때는 **튜닝 스펙** 설정해야한다. 

  - *mtry*: tree를 만들때 몇개의 변수를 고려할것인지. 열에서 종속변수를 제외한 선택할 수 있는 변수의 수
  
  - *min_n*: final node에 몇개의 observation이 있을때까지 split할것인지
  
*mtry*와 *min_n*를 어떻게 정할지를 평가셋을 통해서 결정

->`tune()`를 사용해서 tidymodels에게 알려주도록 한다.

```{r}
cores <- parallel::detectCores() -1
cores
```
**rand_forest()**

  - *tune_spec*: 몇개의 변수를 사용할지 가장 좋은 값으로 정할거임
  
  - *min_n = tune()*: terminal node에 몇개가 남을때가 가장 좋은지 정할거임
  
  - *trees*: 1000개의 tree를 사용할거임
  
  - *set_engine*: ranger package안에 있는 random forest를 쓸거여
  
  - *set_mode*: 지금은 classification 문제다!

```{r}
tune_spec <- rand_forest(mtry = tune(),
                         min_n = tune(),
                         trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

```

각 설정된 *param_grid*안에 위에서 설정한 1000개의 tree가 들어있는거임!!!

  - *grid_random*: tune을 하고싶은 mtry, min_n의 조합으로 만들수 있는 size=?개의 형태를 tibble로 만들어라

  - *grid_regular*: 가질 수 있는 가장 작은 값~큰값 중에 골고루 고르는 것

  - *filter*: mtry가 3이상인것만 고르겠다

```{r}
# param_grid <- grid_latin_hypercube(finalize(mtry(), x = train2[,-1]), min_n(), size = 100)

param_grid <- grid_random(finalize(mtry(), x = train2[,-1]), min_n(), size = 100)

# param_grid <- grid_regular(finalize(mtry(), x = train2[,-1]), min_n(), levels = list(mtry = 10, min_n = 5))

# param_grid$mtry %>% unique()

param_grid %<>% filter(mtry >= 3)

```



```{r}

workflow <- workflow() %>%
  add_model(tune_spec) %>% 
  add_formula(credit ~ .)
workflow

```

  - *tune_grid*: 알맞은 최적의 grid값(parameter)을 찾아라 

**시간이 오래 걸려서 생략**

```{r}
# library(tictoc)
# tic()
# tune_result <- workflow %>% 
#   tune_grid(validation_split,
#             grid = param_grid,
#             metrics = metric_set(mn_log_loss))
# toc()
```

최적의 값을 찾았으면 best인 parameter의 값(mtry, min_n)이 무엇인지

```{r}
# tune_best <- tune_result %>% select_best(metric = "mn_log_loss") #mn_log_loss를 기준으로 best값을 저장해ㅈ
# tune_best$mtry
# tune_best$min_n
```

### mn_log_loss 기준 tunebest -> mtry : 3, min_n : 11


### 2) Random Forest vfold 계산

*mrty*와 *min_n*은 앞에서 구한 best 값을 사용한다.

```{r}
model_spec <- rand_forest(mtry = 3,
                          min_n = 11,
                          trees = 1000) %>% 
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_model(model_spec) %>% 
  add_formula(credit ~ .)

rf_grid <- tibble(mtry=c(3),min_n=c(11))

# library(tictoc)
# tic()
# rf_fit_vfold <-  rf_workflow %>% 
#   tune_grid(validation_split,
#             grid = rf_grid,
#             metrics = metric_set(mn_log_loss),
#             control = control_stack_resamples())
# toc()
```

이렇게해서 stacking에 사용될 random forest vfold를 만들었습니다~!!










