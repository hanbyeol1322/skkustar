[
  {
    "path": "posts/2021-05-19-lec7/",
    "title": "Tree Building, Prunning",
    "description": "lec7",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-05-19",
    "categories": [],
    "contents": "\n어떤 x를 기준으로 나눠야 하는가?(평균 하나로 할 때)\n\n\nlibrary(magrittr)\n\nx <- -10:10\ny <- x^2\n\n\n\nwhich(x < s) : x가 s보다 작은 것들의 위치는 어디니? x[group1_index] : 실제로 x가 s보다 작은 것들이 뭔지를 보여줌\n예측값으로 사용할 것은 group별로 y값들의 평균임!\n\n\ns <- -2\ngroup1_index <- which(x < s)\ngroup2_index <- which(x >= s)\n\nx[group1_index]\n\n\n[1] -10  -9  -8  -7  -6  -5  -4  -3\n\nx[group2_index]\n\n\n [1] -2 -1  0  1  2  3  4  5  6  7  8  9 10\n\npred_group1 <- mean(y[group1_index])\npred_group2 <- mean(y[group2_index])\n\n#rss들의 합\nresult <- sum((y[group1_index] - pred_group1)^2)+\n    sum((y[group2_index] - pred_group2)^2)\n\n\n\n이제 저 것들을 s, x, y의 함수로 만들 것임\n\n\nrss <- function(s, x, y){\n    group1_index <- which(x < s)\n    group2_index <- which(x >= s)\n    \n    x[group1_index]\n    x[group2_index]\n    \n    pred_group1 <- mean(y[group1_index])\n    pred_group2 <- mean(y[group2_index])\n    \n    result <- sum((y[group1_index] - pred_group1)^2)+\n    sum((y[group2_index] - pred_group2)^2)\n    result\n}\n\n\n\ns의 후보군들(?) 정하고 그런 s들로 rss를 구하고자 한다.\n\n\n# x랑 y를 고정시키고 s만 바뀐다고 생각\n\nrss <- function(s){\n    x <- -10:10\n    y <- x^2\n    \n    group1_index <- which(x < s)\n    group2_index <- which(x >= s)\n    \n    x[group1_index]\n    x[group2_index]\n    \n    pred_group1 <- mean(y[group1_index])\n    pred_group2 <- mean(y[group2_index])\n    \n    result <- sum((y[group1_index] - pred_group1)^2)+\n    sum((y[group2_index] - pred_group2)^2)\n    result\n}\nrss(-2)\n\n\n[1] 20916\n\ns <- seq(-10, 10, by = 0.01)\nrss_result <- sapply(s,rss)\n\n\n\nsapply(x,y) : x의 모든 원소들을 rss라는 함수에 입력값으로 넣어라\n\n\nplot(s, rss_result)\n\n\n\nwhich.min(rss_result)\n\n\n[1] 202\n\ns[which.min(rss_result)]\n\n\n[1] -7.99\n\nrss_result를 그려라 which.min(rss_result): rss_result의 몇번째 값이 가장 작은 값인지 확인해보기 s[which.min(rss_result)] : 그때의 s값은 뭐니?\n그럼 그 s값을 기준으로 그룹이 나뉘니까 어떤 값이 예측값으로 사용될 것인지 알 수 있다.\n\n\ngroup1_index <- which(x < -7.99)\ngroup2_index <- which(x >= -7.99)\n    \nmean(y[group1_index])\n\n\n[1] 81.66667\n\nmean(y[group2_index])\n\n\n[1] 29.16667\n\nplot(x,y)\nabline(h=mean(y[group1_index]))\nabline(h=mean(y[group2_index]))\nabline(v=-7.99)\n\n\n\n\n왼쪽 그룹은 81.667로 예측을하고 오른쪽 그룹은 29.16667로 예측한다는 것을 알 수 있다.\noverfitting을 방지하기 위해서는 어째야함?\nlinear regression에서는 ridge, lasso에 penalty를 부과하는 방법을 사용했음 그럼 tree에서는? -> prunning\nR에서 tree하는 방법\n\n\nlibrary(ISLR)\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(rpart)\nlibrary(rpart.plot) #tree 시각화 하는 패키지\n\n\n\n\n\nattach(Boston)\n\nBoston %>% dim()\n\n\n[1] 506  14\n\nBoston %>% head()\n\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7\n   black lstat medv\n1 396.90  4.98 24.0\n2 396.90  9.14 21.6\n3 392.83  4.03 34.7\n4 394.63  2.94 33.4\n5 396.90  5.33 36.2\n6 394.12  5.21 28.7\n\ndecision tree building, Purnning 방법\nregression 할때는 lm()에 넣었는데 decision tree에서는 rpart() 함수에 넣음\n\n\nboston_tree <- rpart(medv ~ .,\n                     data = Boston,\n                     method = \"anova\", #rpart에서는 anova로 regression을 적용\n                     control = rpart.control(\n                       cp = 0, #anova split일 때: 나눌 때마다 R^2가 증가할텐데, 증가하는 정도를 어느정도로 할것인지\n                       #cp가 높을수록 tree구조가 단순해진다\n                       minbucket = 1, #terminal node가 1개 일 때까지 building\n                       maxdepth = 10))\nrpart.plot(boston_tree)\n\n\n\n\nrpart.control 함수에 자동으로 xval하는 것이 저장되어있음(validation 진행)\n\n\nclass(boston_tree$cptable)\n\n\n[1] \"matrix\" \"array\" \n\nmatrix니까\n\n\nplot(1:166, boston_tree$cptable[,3], type = \"l\") #cp에 따라서 rel error가 어떻게 바뀌는지(train set에 대한 error값-쭉 떨어짐)\npoints(1:166, boston_tree$cptable[,4],type = \"l\", col=\"red\") #cp에 따라서 cross val가 어떻게 바뀌는지(cross validation에 대한 error값-가장 낮은 시점부터 overfitting이 시작됨)\n\n\n\n\ncp값에 따라서 prune을 할 수 있다\n\n\nprintcp(boston_tree) \n\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = Boston, method = \"anova\", control = rpart.control(cp = 0, \n    minbucket = 1, maxdepth = 10))\n\nVariables actually used in tree construction:\n [1] age     black   chas    crim    dis     indus   lstat   nox    \n [9] ptratio rm      tax     zn     \n\nRoot node error: 42716/506 = 84.42\n\nn= 506 \n\n            CP nsplit rel error  xerror     xstd\n1   4.5274e-01      0  1.000000 1.00601 0.083470\n2   1.7117e-01      1  0.547256 0.63285 0.058525\n3   7.1658e-02      2  0.376083 0.41149 0.046188\n4   5.9002e-02      3  0.304426 0.36616 0.044983\n5   3.3756e-02      4  0.245424 0.30330 0.040041\n6   2.6613e-02      5  0.211668 0.28547 0.040105\n7   2.3572e-02      6  0.185055 0.26062 0.039774\n8   1.3031e-02      7  0.161483 0.24977 0.039702\n9   9.1470e-03      8  0.148452 0.24258 0.038392\n10  7.4304e-03      9  0.139305 0.23876 0.038344\n11  7.2654e-03     10  0.131874 0.27516 0.052212\n12  7.0714e-03     11  0.124609 0.27706 0.052244\n13  6.1263e-03     12  0.117537 0.27543 0.052248\n14  4.8053e-03     13  0.111411 0.27289 0.052269\n15  4.5609e-03     14  0.106606 0.26983 0.052030\n16  4.2593e-03     15  0.102045 0.26981 0.052065\n17  3.9410e-03     16  0.097785 0.27027 0.052070\n18  3.6633e-03     17  0.093844 0.26612 0.051543\n19  3.6603e-03     18  0.090181 0.27029 0.051674\n20  3.1933e-03     19  0.086521 0.26825 0.051670\n21  3.0157e-03     20  0.083328 0.26886 0.051748\n22  2.8092e-03     21  0.080312 0.26575 0.051704\n23  2.3613e-03     22  0.077503 0.26948 0.052113\n24  2.2459e-03     23  0.075141 0.28070 0.053126\n25  2.2354e-03     25  0.070649 0.28147 0.053123\n26  2.1590e-03     26  0.068414 0.28074 0.053123\n27  2.1525e-03     27  0.066255 0.27962 0.053120\n28  2.0923e-03     28  0.064103 0.27871 0.053122\n29  1.7169e-03     29  0.062010 0.28044 0.053126\n30  1.6755e-03     30  0.060293 0.28029 0.053182\n31  1.6210e-03     31  0.058618 0.28218 0.053176\n32  1.3829e-03     32  0.056997 0.28233 0.053176\n33  1.3690e-03     33  0.055614 0.28517 0.053239\n34  1.3683e-03     35  0.052876 0.28516 0.053239\n35  1.3616e-03     37  0.050139 0.28520 0.053239\n36  1.3140e-03     38  0.048778 0.26809 0.048089\n37  1.2266e-03     39  0.047464 0.26789 0.048089\n38  1.1922e-03     40  0.046237 0.26658 0.048019\n39  1.1395e-03     41  0.045045 0.26894 0.048031\n40  1.1373e-03     42  0.043905 0.26841 0.048033\n41  8.7789e-04     43  0.042768 0.26775 0.047975\n42  8.7588e-04     44  0.041890 0.27164 0.048191\n43  8.6854e-04     45  0.041014 0.27182 0.048190\n44  8.3959e-04     46  0.040146 0.27197 0.048192\n45  8.3249e-04     47  0.039306 0.27177 0.048193\n46  7.8267e-04     48  0.038474 0.27208 0.048196\n47  7.3876e-04     49  0.037691 0.27160 0.048196\n48  7.2936e-04     50  0.036952 0.27316 0.048186\n49  7.0451e-04     51  0.036223 0.27254 0.047658\n50  6.8467e-04     52  0.035519 0.27388 0.047699\n51  6.4293e-04     53  0.034834 0.27283 0.047677\n52  6.4238e-04     54  0.034191 0.27405 0.047677\n53  6.2996e-04     55  0.033549 0.27405 0.047677\n54  6.2702e-04     56  0.032919 0.27405 0.047677\n55  6.0453e-04     57  0.032292 0.27361 0.047682\n56  5.9700e-04     58  0.031687 0.27310 0.047673\n57  5.6185e-04     59  0.031090 0.27308 0.047679\n58  5.5923e-04     60  0.030528 0.27713 0.047799\n59  5.1245e-04     62  0.029410 0.27863 0.047814\n60  5.0523e-04     63  0.028897 0.27838 0.047820\n61  5.0046e-04     64  0.028392 0.27730 0.047819\n62  4.8701e-04     65  0.027892 0.27654 0.047799\n63  4.5515e-04     66  0.027405 0.27568 0.047794\n64  4.3840e-04     67  0.026949 0.27568 0.047799\n65  4.3262e-04     68  0.026511 0.27537 0.047794\n66  4.3016e-04     69  0.026078 0.27585 0.047942\n67  4.2266e-04     70  0.025648 0.27594 0.047943\n68  4.2107e-04     71  0.025226 0.27609 0.047943\n69  4.0174e-04     72  0.024804 0.27582 0.047959\n70  3.9534e-04     73  0.024403 0.27651 0.048216\n71  3.8774e-04     74  0.024007 0.27663 0.048215\n72  3.8700e-04     75  0.023620 0.27685 0.048213\n73  3.5562e-04     76  0.023233 0.27676 0.048209\n74  3.5350e-04     78  0.022521 0.27781 0.048199\n75  3.3828e-04     79  0.022168 0.27812 0.048199\n76  3.3361e-04     80  0.021830 0.27972 0.048982\n77  3.0902e-04     81  0.021496 0.27878 0.048953\n78  3.0117e-04     82  0.021187 0.27918 0.048949\n79  2.9874e-04     85  0.020283 0.27909 0.048947\n80  2.8486e-04     86  0.019985 0.27881 0.048943\n81  2.8354e-04     87  0.019700 0.27759 0.048903\n82  2.6547e-04     88  0.019416 0.27756 0.048903\n83  2.4856e-04     89  0.019151 0.27761 0.048910\n84  2.4340e-04     90  0.018902 0.27772 0.048909\n85  2.2630e-04     91  0.018659 0.27798 0.048912\n86  2.0268e-04     92  0.018433 0.27755 0.048910\n87  2.0227e-04     94  0.018027 0.27790 0.048933\n88  1.9700e-04     95  0.017825 0.27763 0.048935\n89  1.9283e-04     96  0.017628 0.27733 0.048936\n90  1.9278e-04     97  0.017435 0.27722 0.048936\n91  1.8150e-04     98  0.017242 0.27753 0.048933\n92  1.7093e-04    100  0.016879 0.27855 0.048932\n93  1.6931e-04    101  0.016708 0.27884 0.048936\n94  1.6512e-04    102  0.016539 0.27876 0.048937\n95  1.5957e-04    103  0.016374 0.27772 0.048932\n96  1.5899e-04    104  0.016214 0.27695 0.048911\n97  1.5802e-04    105  0.016055 0.27695 0.048911\n98  1.4998e-04    106  0.015897 0.27737 0.048912\n99  1.4646e-04    107  0.015747 0.27717 0.048912\n100 1.4622e-04    108  0.015601 0.27717 0.048912\n101 1.3582e-04    109  0.015455 0.27875 0.049481\n102 1.3578e-04    110  0.015319 0.27875 0.049481\n103 1.2800e-04    111  0.015183 0.27842 0.049474\n104 1.2660e-04    112  0.015055 0.27853 0.049475\n105 1.1803e-04    113  0.014929 0.27859 0.049479\n106 1.1803e-04    114  0.014811 0.27882 0.049479\n107 1.1464e-04    115  0.014693 0.27882 0.049479\n108 1.1268e-04    116  0.014578 0.27917 0.049507\n109 1.0396e-04    117  0.014465 0.27892 0.049508\n110 9.5592e-05    118  0.014361 0.27871 0.049508\n111 9.5592e-05    119  0.014266 0.27830 0.049509\n112 8.9018e-05    120  0.014170 0.27820 0.049509\n113 8.8491e-05    121  0.014081 0.27797 0.049508\n114 8.7312e-05    122  0.013993 0.27797 0.049508\n115 8.5215e-05    123  0.013905 0.27797 0.049509\n116 8.4979e-05    124  0.013820 0.27797 0.049509\n117 8.2424e-05    125  0.013735 0.27839 0.049508\n118 7.8303e-05    126  0.013653 0.27837 0.049508\n119 7.5849e-05    128  0.013496 0.27848 0.049515\n120 7.3984e-05    129  0.013420 0.27843 0.049517\n121 7.1916e-05    131  0.013272 0.27870 0.049522\n122 7.1346e-05    132  0.013200 0.27867 0.049523\n123 7.0426e-05    133  0.013129 0.27870 0.049523\n124 6.8826e-05    134  0.013059 0.27904 0.049523\n125 5.9406e-05    135  0.012990 0.27692 0.049451\n126 5.6341e-05    136  0.012930 0.27735 0.049449\n127 5.6243e-05    137  0.012874 0.27737 0.049448\n128 5.4800e-05    138  0.012818 0.27737 0.049448\n129 4.7944e-05    139  0.012763 0.27744 0.049448\n130 4.7476e-05    140  0.012715 0.27754 0.049447\n131 4.4481e-05    141  0.012667 0.27788 0.049450\n132 4.3894e-05    142  0.012623 0.27838 0.049457\n133 4.2490e-05    143  0.012579 0.27839 0.049458\n134 4.1954e-05    144  0.012537 0.27839 0.049458\n135 3.9954e-05    145  0.012495 0.27834 0.049459\n136 3.9954e-05    146  0.012455 0.27787 0.049457\n137 3.7495e-05    147  0.012415 0.27778 0.049456\n138 3.7495e-05    148  0.012377 0.27778 0.049456\n139 3.4132e-05    149  0.012340 0.27783 0.049459\n140 3.2813e-05    151  0.012271 0.27779 0.049459\n141 3.0960e-05    152  0.012239 0.27781 0.049459\n142 2.4386e-05    153  0.012208 0.27811 0.049462\n143 2.4386e-05    154  0.012183 0.27798 0.049463\n144 2.2474e-05    155  0.012159 0.27809 0.049462\n145 2.1128e-05    156  0.012136 0.27810 0.049462\n146 1.8748e-05    157  0.012115 0.27826 0.049462\n147 1.6407e-05    158  0.012097 0.27833 0.049457\n148 1.5607e-05    159  0.012080 0.27831 0.049457\n149 1.5607e-05    160  0.012065 0.27834 0.049457\n150 1.4085e-05    161  0.012049 0.27834 0.049457\n151 1.1276e-05    162  0.012035 0.27846 0.049457\n152 1.1237e-05    163  0.012024 0.27848 0.049457\n153 9.4421e-06    164  0.012012 0.27850 0.049457\n154 7.6474e-06    165  0.012003 0.27834 0.049457\n155 6.5939e-06    166  0.011995 0.27866 0.049463\n156 5.6380e-06    167  0.011989 0.27866 0.049463\n157 5.6185e-06    168  0.011983 0.27866 0.049463\n158 4.8771e-06    169  0.011977 0.27875 0.049462\n159 4.7211e-06    170  0.011973 0.27875 0.049462\n160 3.9017e-06    171  0.011968 0.27876 0.049462\n161 3.8237e-06    172  0.011964 0.27882 0.049462\n162 3.1604e-06    173  0.011960 0.27882 0.049462\n163 9.7543e-07    174  0.011957 0.27882 0.049462\n164 6.2427e-07    175  0.011956 0.27880 0.049462\n165 1.5607e-07    176  0.011955 0.27880 0.049462\n166 0.0000e+00    177  0.011955 0.27880 0.049462\n\nspli할수록 rel error가 작아짐(train data에 대해서 학습을 더 잘한다고 볼 수 있음) xerror: cross validation error: train data로 학습할 때 xerror가 감소하다가 overfit되는 지점부터는 다시 증가함\n우리의 목적: cp중에서 cross validarion이 작고, overfit되지 않은 값을 찾기를 원함\ncp중에서 cross validarion이 작은 것이 어디있니: which.min():index를 찾아내고 그때의 cp값을 잡아옴\n\n\nbestcp <- boston_tree$cptable[which.min(boston_tree$cptable[,\"xerror\"]),\"CP\"]\n\n\n\n위에서 정한 cp를 기준으로 prune을 해라.\n\n\nbest_boston_tree <- prune(boston_tree, cp = bestcp)\nrpart.plot(best_boston_tree)\n\n\n\n\n복잡한 tree에서 prune을 하는 이유: optimal tree를 찾는 것이 목적인데, Gini index와 Entropy를 기준으로 설정하는 것보다 cp를 이용해서 prunning을 하는 방법이 좋은 tree를 찾는다고 알려져있음(in textbook) +남들에게 설명해주기 편함(현상해석을 분석할 때 어덯게 왜 작동하는지) -> predict 모델에서는 이유를 설명하기 보다는 잘 추정하는 것이 더 중요함\n\n\n\n",
    "preview": "posts/2021-05-19-lec7/lec7_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-05-19T04:41:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-15-hw7/",
    "title": "hw7",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\n\nContents\nhw6에 이어 Ridge, Lasso penalty를 시각적으로 표현함!!(밑에부터 달라짐)\n1. Outlier 제거\n1.1 Libraries\n1.2 Data load\n1.3 Outliers 확인하기\n\n2. training\n2.1 Elastic net\n2.1.1 Ridge Regression\n\n\n\nhw6에 이어 Ridge, Lasso penalty를 시각적으로 표현함!!(밑에부터 달라짐)\n1. Outlier 제거\noutlier를 제거하기 위해 데이터를 다시 불러와서 시작한당 준비단계….\n1.1 Libraries\n\n\ncode\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(skimr)\nlibrary(knitr)\ntheme_set(theme_bw())\n\n\n\n1.2 Data load\n\n\ncode\n\nfile_path <- \"/cloud/project\"\nfiles <- list.files(file_path)\nfiles\n\n\n [1] \"_posts\"                \"_site.yml\"            \n [3] \"about.Rmd\"             \"blog_posts.Rmd\"       \n [5] \"docs\"                  \"index.Rmd\"            \n [7] \"Portfolio.Rmd\"         \"postcard.R\"           \n [9] \"project.Rproj\"         \"README.md\"            \n[11] \"sample_submission.csv\" \"test.csv\"             \n[13] \"tobi.jpg\"              \"train.csv\"            \n[15] \"뭐에 쓴거임.Rmd\"       \"프사.jpg\"             \n\n\n\ncode\n\ntest <- read_csv(file.path(file_path, \"test.csv\"))%>% \n  janitor::clean_names()\ntrain <- read_csv(file.path(file_path, \"train.csv\"))%>% \n  janitor::clean_names() \n\n\n\n1.3 Outliers 확인하기\ngr_liv_area 변수에 있는 outlier를 확인해보자!\nAny thoughts about right 4 points?\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = gr_liv_area, \n             y = log(sale_price))) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Before removing outliers\")\n\n\n\n\n4500이상 되는 값들이 유독 튀는 것을 확인 할 수 있다. 4500 이상 되는 2개의 값을 필터 씌운 뒤에 다시 train data로 넣어보자 %<>%\n\n\ncode\n\n# outliers \ntrain %>%\n  filter(gr_liv_area > 4500) %>%\n  DT::datatable(width = \"100%\",  \n                options = list(scrollX = TRUE))\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\"],[524,1299],[60,60],[\"RL\",\"RL\"],[130,313],[40094,63887],[\"Pave\",\"Pave\"],[null,null],[\"IR1\",\"IR3\"],[\"Bnk\",\"Bnk\"],[\"AllPub\",\"AllPub\"],[\"Inside\",\"Corner\"],[\"Gtl\",\"Gtl\"],[\"Edwards\",\"Edwards\"],[\"PosN\",\"Feedr\"],[\"PosN\",\"Norm\"],[\"1Fam\",\"1Fam\"],[\"2Story\",\"2Story\"],[10,10],[5,5],[2007,2008],[2008,2008],[\"Hip\",\"Hip\"],[\"CompShg\",\"ClyTile\"],[\"CemntBd\",\"Stucco\"],[\"CmentBd\",\"Stucco\"],[\"Stone\",\"Stone\"],[762,796],[\"Ex\",\"Ex\"],[\"TA\",\"TA\"],[\"PConc\",\"PConc\"],[\"Ex\",\"Ex\"],[\"TA\",\"TA\"],[\"Gd\",\"Gd\"],[\"GLQ\",\"GLQ\"],[2260,5644],[\"Unf\",\"Unf\"],[0,0],[878,466],[3138,6110],[\"GasA\",\"GasA\"],[\"Ex\",\"Ex\"],[\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\"],[3138,4692],[1538,950],[0,0],[4676,5642],[1,2],[0,0],[3,2],[1,1],[3,3],[1,1],[\"Ex\",\"Ex\"],[11,12],[\"Typ\",\"Typ\"],[1,3],[\"Gd\",\"Gd\"],[\"BuiltIn\",\"Attchd\"],[2007,2008],[\"Fin\",\"Fin\"],[3,2],[884,1418],[\"TA\",\"TA\"],[\"TA\",\"TA\"],[\"Y\",\"Y\"],[208,214],[406,292],[0,0],[0,0],[0,0],[0,480],[null,\"Gd\"],[null,null],[null,null],[0,0],[10,1],[2007,2008],[\"New\",\"New\"],[\"Partial\",\"Partial\"],[184750,160000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\ncode\n\n# outliers remove\ntrain %<>% filter(!(gr_liv_area > 4500))\n\n\n\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = gr_liv_area, \n             y = log(sale_price))) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"After removing outliers\")\n\n\n\n\noutlier를 지우니 추세가 눈에 확 들어온다!\ntotal_bsmt_sf 변수에 있는 outlier를 확인해보자!\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = total_bsmt_sf, \n             y = log(sale_price))) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Before removing outliers\")\n\n\n\n\ntotal_bsmt_sf가 3000 이상 되는 값들이 튀는 것처럼 보인다.\noutlier점 3개중에 2개를 지웠다. (왜 로그값이 13보다 작은 값들을 지우셨는지는 좀 더 생각해보자)\n\n\ncode\n\n# outliers \ntrain %>%\n  filter(total_bsmt_sf > 3000 &\n                   log(sale_price) < 13) %>%\n  DT::datatable(width = \"100%\",  \n                options = list(scrollX = TRUE))\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\"],[333,497],[20,20],[\"RL\",\"RL\"],[85,null],[10655,12692],[\"Pave\",\"Pave\"],[null,null],[\"IR1\",\"IR1\"],[\"Lvl\",\"Lvl\"],[\"AllPub\",\"AllPub\"],[\"Inside\",\"Inside\"],[\"Gtl\",\"Gtl\"],[\"NridgHt\",\"NoRidge\"],[\"Norm\",\"Norm\"],[\"Norm\",\"Norm\"],[\"1Fam\",\"1Fam\"],[\"1Story\",\"1Story\"],[8,8],[5,5],[2003,1992],[2004,1993],[\"Gable\",\"Hip\"],[\"CompShg\",\"CompShg\"],[\"VinylSd\",\"BrkFace\"],[\"VinylSd\",\"BrkFace\"],[\"BrkFace\",\"None\"],[296,0],[\"Gd\",\"Gd\"],[\"TA\",\"TA\"],[\"PConc\",\"PConc\"],[\"Gd\",\"Gd\"],[\"TA\",\"TA\"],[\"No\",\"No\"],[\"GLQ\",\"GLQ\"],[1124,1231],[null,\"Unf\"],[479,0],[1603,1969],[3206,3200],[\"GasA\",\"GasA\"],[\"Ex\",\"Ex\"],[\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\"],[1629,3228],[0,0],[0,0],[1629,3228],[1,1],[0,0],[2,3],[0,0],[3,4],[1,1],[\"Gd\",\"Gd\"],[7,10],[\"Typ\",\"Typ\"],[1,1],[\"Gd\",\"Gd\"],[\"Attchd\",\"Attchd\"],[2003,1992],[\"RFn\",\"RFn\"],[3,2],[880,546],[\"TA\",\"TA\"],[\"TA\",\"TA\"],[\"Y\",\"Y\"],[0,264],[0,75],[0,291],[0,0],[0,0],[0,0],[null,null],[null,null],[null,null],[0,0],[10,5],[2009,2007],[\"WD\",\"WD\"],[\"Normal\",\"Normal\"],[284000,430000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\ncode\n\n# outliers remove\ntrain %<>% filter(!(total_bsmt_sf > 3000 &\n                   log(sale_price) < 13))\n\n\n\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = total_bsmt_sf, \n             y = log(sale_price))) +\n  geom_point() +\n  labs(title = \"After removing outliers\")\n\n\n\n\n잘 지워졌당~!\ngarage_area 변수에 있는 outlier를 확인해보자!\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = garage_area, \n             y = log(sale_price))) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Before removing outliers\")\n\n\n\n\n1230보다 큰 3개의 점이 거슬린다.. 저걸 지워보쟈\n\n\ncode\n\n# outliers \ntrain %>% \n  filter(garage_area > 1230) %>%\n  DT::datatable(width = \"100%\",  \n                options = list(scrollX = TRUE))\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\"],[582,1062,1191],[20,30,190],[\"RL\",\"C (all)\",\"RL\"],[98,120,null],[12704,18000,32463],[\"Pave\",\"Grvl\",\"Pave\"],[null,null,null],[\"Reg\",\"Reg\",\"Reg\"],[\"Lvl\",\"Low\",\"Low\"],[\"AllPub\",\"AllPub\",\"AllPub\"],[\"Inside\",\"Inside\",\"Inside\"],[\"Gtl\",\"Gtl\",\"Mod\"],[\"NridgHt\",\"IDOTRR\",\"Mitchel\"],[\"Norm\",\"Norm\",\"Norm\"],[\"Norm\",\"Norm\",\"Norm\"],[\"1Fam\",\"1Fam\",\"2fmCon\"],[\"1Story\",\"1Story\",\"1Story\"],[8,3,4],[5,4,4],[2008,1935,1961],[2009,1950,1975],[\"Hip\",\"Gable\",\"Gable\"],[\"CompShg\",\"CompShg\",\"CompShg\"],[\"VinylSd\",\"MetalSd\",\"MetalSd\"],[\"VinylSd\",\"MetalSd\",\"MetalSd\"],[\"BrkFace\",\"None\",\"Stone\"],[306,0,149],[\"Ex\",\"Fa\",\"TA\"],[\"TA\",\"TA\",\"Gd\"],[\"PConc\",\"CBlock\",\"CBlock\"],[\"Ex\",\"TA\",\"TA\"],[\"TA\",\"TA\",\"TA\"],[\"No\",\"No\",\"Av\"],[\"Unf\",\"Unf\",\"BLQ\"],[0,0,1159],[\"Unf\",\"Unf\",\"Unf\"],[0,0,0],[2042,894,90],[2042,894,1249],[\"GasA\",\"GasA\",\"GasA\"],[\"Ex\",\"TA\",\"Ex\"],[\"Y\",\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\",\"SBrkr\"],[2042,894,1622],[0,0,0],[0,0,0],[2042,894,1622],[0,0,1],[0,0,0],[2,1,1],[1,0,0],[3,2,3],[1,1,1],[\"Ex\",\"TA\",\"TA\"],[8,6,7],[\"Typ\",\"Typ\",\"Typ\"],[1,0,1],[\"Gd\",null,\"TA\"],[\"Attchd\",\"Detchd\",\"2Types\"],[2009,1994,1975],[\"RFn\",\"RFn\",\"Fin\"],[3,3,4],[1390,1248,1356],[\"TA\",\"TA\",\"TA\"],[\"TA\",\"TA\",\"TA\"],[\"Y\",\"Y\",\"Y\"],[0,0,439],[90,20,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[null,null,null],[null,null,null],[null,\"Shed\",null],[0,560,0],[8,8,3],[2009,2008,2007],[\"New\",\"ConLD\",\"WD\"],[\"Partial\",\"Normal\",\"Normal\"],[253293,81000,168000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\ncode\n\n# outliers remove\ntrain %<>% filter(garage_area <= 1230)\n\n\n\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = garage_area, \n             y = log(sale_price))) +\n  geom_point() +\n  labs(title = \"After removing outliers\")\n\n\n\n\n잘 지워졌구나 ~!\n2. training\n이제 거슬리는 outliers을 지웠으니 학습을 통해 예측력을 높여보자 !!!\n1번에서 outliers를 제거한 train set을 이용해서 training 해볼 것이다.!\n먼저 outlier를 제거한 train과 test를 다시 전처리해준다.\n\n\ncode\n\nall_data <- bind_rows(train, test)\nnames(all_data)[1:10]\n\n\n [1] \"id\"           \"ms_sub_class\" \"ms_zoning\"    \"lot_frontage\"\n [5] \"lot_area\"     \"street\"       \"alley\"        \"lot_shape\"   \n [9] \"land_contour\" \"utilities\"   \n\nMake recipe\n\n\ncode\n\nhousing_recipe <- all_data %>% \n  recipe(sale_price ~ .) %>%\n  step_rm(id) %>% \n  step_log(sale_price) %>%\n  step_modeimpute(all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_meanimpute(all_predictors()) %>%\n  step_normalize(all_predictors()) %>% \n  prep(training = all_data)\nprint(housing_recipe)\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2912 data points and 2912 incomplete rows. \n\nOperations:\n\nVariables removed id [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, alley, ... [trained]\nDummy variables from ms_zoning, street, alley, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\njuice the all_data2 and split\n\n\ncode\n\nall_data2 <- juice(housing_recipe)\n\n\n\nWe are done for preprocessing. Let’s split the data set.\n\n\ncode\n\ntrain_index <- seq_len(nrow(train))\ntrain2 <- all_data2[train_index,]\ntest2 <- all_data2[-train_index,]\n\n\n\n전처리가 다 끝났으면 이제 어떻게 학습할 것인지 알아보자\n2.1 Elastic net\ntrain set을 validation set으로 다시 분리 시킬 것이다. 70%는 학습용으로 남겨놓아라. 30%는 평가용!!\n\n\ncode\n\nset.seed(2021)\nvalidation_split <- validation_split(train2, prop = 0.7)\n#validation_split <- vfold_cv(train2, v = 10, strata = sale_price)\n# actual split id stored in the following\n# validation_split$splits[[1]]$in_id\n# the whole point is that it's there and trust tidymodels :)\n# head(validation_split$splits[[1]]$in_id)\n\n\n\n이것이 실제로 어떻게 나눠져 있는지 확인하려면\nin_id : 학습 할 때 사용하는 것 out_id : 평가 할 때 사용하는 것\n\n\ncode\n\nvalidation_split$splits[[1]]$in_id\n\n\n   [1]    1    2    3    4    5    6    9   11   13   14   15   16\n  [13]   18   19   21   22   23   24   25   26   28   30   31   32\n  [25]   33   34   35   36   37   38   39   40   42   45   46   50\n  [37]   51   52   54   56   58   61   62   64   65   66   67   68\n  [49]   69   71   72   73   74   78   80   82   83   85   86   87\n  [61]   88   90   91   92   94   96   97   99  100  102  104  105\n  [73]  106  108  109  110  111  112  114  115  117  119  120  121\n  [85]  122  124  125  126  128  130  131  132  133  135  136  137\n  [97]  138  140  141  142  143  144  145  146  147  149  150  151\n [109]  152  153  157  158  159  160  161  162  163  165  168  169\n [121]  170  171  173  176  180  181  182  183  184  186  188  189\n [133]  190  193  194  195  196  197  200  201  202  203  204  206\n [145]  207  208  209  210  211  212  213  214  215  216  217  218\n [157]  219  220  221  222  223  224  225  228  229  231  233  234\n [169]  236  237  238  239  242  243  244  245  247  248  249  250\n [181]  252  253  255  256  257  259  260  261  263  264  265  266\n [193]  267  268  269  270  271  272  273  274  277  280  281  283\n [205]  284  286  287  288  289  290  291  292  293  296  297  298\n [217]  299  301  303  304  308  309  310  311  314  315  316  318\n [229]  319  322  323  324  326  328  329  330  331  332  333  334\n [241]  335  336  337  338  343  345  346  347  348  349  352  353\n [253]  354  355  356  357  358  359  363  364  366  367  368  370\n [265]  371  372  375  376  377  378  379  380  381  382  383  384\n [277]  386  389  391  393  394  397  398  399  401  404  405  407\n [289]  408  409  410  411  412  413  414  415  416  417  418  419\n [301]  421  422  423  424  425  426  428  429  431  433  434  435\n [313]  437  438  439  440  441  443  444  447  448  450  452  455\n [325]  456  457  459  460  462  463  464  466  467  468  469  470\n [337]  473  474  476  477  478  479  480  482  483  484  486  487\n [349]  488  490  493  494  496  497  498  499  500  501  502  505\n [361]  506  507  508  509  512  514  515  516  517  518  519  520\n [373]  521  524  525  526  527  528  529  530  531  532  533  534\n [385]  536  537  539  540  541  542  544  545  546  547  548  549\n [397]  552  553  554  555  556  557  558  559  560  561  562  563\n [409]  564  565  566  567  568  570  571  572  573  574  575  576\n [421]  578  579  582  584  585  586  587  588  589  590  591  592\n [433]  593  594  595  596  597  598  603  604  605  610  612  613\n [445]  614  615  617  618  619  620  621  623  626  627  629  630\n [457]  631  633  634  635  636  638  640  641  643  644  646  647\n [469]  648  650  652  654  656  657  658  659  662  663  665  668\n [481]  672  675  676  677  679  684  685  688  689  690  691  692\n [493]  693  694  695  696  697  698  699  700  701  702  704  705\n [505]  706  707  708  709  711  715  717  718  719  721  723  725\n [517]  727  730  731  733  734  735  736  738  740  741  744  745\n [529]  746  747  748  749  750  751  752  753  755  756  757  759\n [541]  760  761  763  765  766  767  770  771  772  773  774  775\n [553]  776  777  779  780  781  783  786  787  788  789  791  792\n [565]  794  795  796  798  799  800  802  803  804  805  807  810\n [577]  811  813  815  816  817  818  819  820  822  823  824  825\n [589]  826  828  829  830  832  834  835  837  838  839  840  842\n [601]  845  846  847  848  849  850  851  852  853  854  856  857\n [613]  858  859  860  861  862  863  864  865  867  868  869  871\n [625]  872  874  875  876  877  879  880  882  885  888  889  890\n [637]  891  892  895  896  897  898  901  902  905  906  910  912\n [649]  913  914  916  917  918  920  921  922  923  924  926  927\n [661]  929  930  931  935  936  937  938  940  941  942  943  948\n [673]  949  950  951  952  953  954  955  957  958  959  960  962\n [685]  964  965  966  968  969  971  972  975  976  977  978  981\n [697]  983  984  985  987  988  991  993  994  995  996  997  998\n [709]  999 1001 1002 1003 1005 1008 1010 1011 1013 1014 1016 1017\n [721] 1018 1020 1021 1022 1024 1026 1027 1029 1030 1031 1033 1034\n [733] 1035 1038 1039 1040 1041 1042 1043 1045 1047 1048 1049 1050\n [745] 1053 1054 1055 1056 1057 1059 1060 1061 1062 1063 1064 1066\n [757] 1067 1068 1069 1070 1071 1072 1073 1075 1076 1077 1079 1080\n [769] 1081 1082 1083 1086 1088 1089 1090 1092 1095 1096 1098 1099\n [781] 1100 1101 1102 1103 1104 1105 1107 1108 1110 1111 1112 1113\n [793] 1117 1119 1121 1123 1124 1125 1126 1127 1128 1129 1131 1134\n [805] 1135 1136 1137 1138 1140 1141 1142 1143 1146 1151 1152 1154\n [817] 1155 1156 1158 1159 1160 1161 1162 1163 1165 1169 1171 1173\n [829] 1177 1180 1181 1183 1184 1185 1186 1187 1188 1190 1191 1193\n [841] 1194 1196 1198 1199 1200 1201 1202 1203 1205 1207 1209 1210\n [853] 1211 1213 1214 1216 1217 1218 1220 1223 1224 1225 1226 1228\n [865] 1229 1230 1231 1232 1233 1234 1236 1240 1241 1242 1243 1244\n [877] 1245 1247 1248 1249 1250 1251 1252 1254 1255 1256 1262 1263\n [889] 1265 1266 1267 1268 1269 1270 1272 1273 1275 1276 1278 1279\n [901] 1280 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1294\n [913] 1295 1297 1300 1304 1306 1307 1308 1309 1311 1312 1314 1315\n [925] 1316 1320 1321 1322 1324 1327 1328 1329 1330 1331 1333 1334\n [937] 1335 1337 1338 1339 1340 1341 1342 1343 1346 1347 1349 1350\n [949] 1351 1352 1354 1355 1357 1360 1361 1362 1363 1365 1368 1369\n [961] 1370 1371 1372 1374 1375 1376 1377 1379 1383 1384 1386 1388\n [973] 1389 1390 1391 1392 1394 1395 1398 1399 1400 1401 1402 1404\n [985] 1405 1406 1407 1409 1410 1411 1413 1414 1415 1416 1421 1422\n [997] 1423 1424 1425 1428 1429 1430 1432 1433 1434 1435 1436 1437\n[1009] 1438 1440 1441 1442 1446 1448 1450 1451 1452 1453\n\ncode\n\nhead(validation_split$splits[[1]]$in_id)\n\n\n[1] 1 2 3 4 5 6\n\n2.1.1 Ridge Regression\n이제 tune을 해야하는데 Elastic net에서 \\(\\alpha\\)(=mixture),penalty(=lambda)\nridge regression: Mixture=0\npenalty는 아직 안정했음 : tune 할 것이다.\nSet the tuning spec 람다의 후보군을 여러개 남기기 위해서\ntune_spec: ridge regression에 penalty는 아직 정해지지 않은 상태\nglmnet: elastic net을 사용하기 위한 packaages\nparam_grid : 후보가 될 수 있는 람다의 값이 들어있음(하나하나 학습 시킬 것)0이 없고, 1이 무한대인 상태!\ngrid_regular(levels = 50) : 0에서 1까지 중에 균등하게 50개를 뽑은 것\n\n\ncode\n\ntune_spec <- linear_reg(penalty = tune(),\n                        mixture = 0) %>% #ridge\n  set_engine(\"glmnet\")\nparam_grid <- grid_regular(penalty(), levels = 100)\n\n\n\nSet workflow()\n평가하기 위한 모델을 만들기\n\n\ncode\n\nworkflow <- workflow() %>%\n  add_model(tune_spec) %>% \n  add_formula(sale_price ~ .)\n\n\n\nTuning \\(\\lambda\\) and \\(\\alpha\\) validation_split(학습, test데이터)에다가 param_grid를 fitting한다.\n어떤 것이 좋은지 평가하는 방식은 rmse(soot mean square error)를 이용하겠다.\n-> tune_result에 각 람다 값에 대한 퍼포먼스가 담기게 된다.\n\n\ncode\n\nlibrary(tictoc)\ndoParallel::registerDoParallel()\ntic()\ntune_result <- workflow %>% \n  tune_grid(validation_split,\n            grid = param_grid,\n            metrics = metric_set(rmse))\ntoc()\n\n\n0.908 sec elapsed\n\ncollect_metrics(): 퍼포먼스가 담긴 것들을 모아봐라\nmean 값: rmse값(validation에서 나온 것 들의 평균)\n우리의 목표: mean 값이 최소가 되는 penalty값을 구하는 것\n\n\ncode\n\ntune_result %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 2 1.26e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 3 1.59e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 4 2.01e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 5 2.54e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 6 3.20e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 7 4.04e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 8 5.09e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 9 6.43e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n10 8.11e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n# … with 90 more rows\n\nVisualization of the tunning result\nmixture은 지금 없으니까(ridge) 잠시 나가있어\n\n\ncode\n\ntune_best <- tune_result %>% select_best(metric = \"rmse\")\ntune_best$penalty\n\n\n[1] 0.03853529\n\ncode\n\n# tune_best$mixture\n\n\n\ntune_result안네 penalty대비 rmse의 값을 보여주는 plot\n\n\ncode\n\ntune_result %>%\n  collect_metrics() %>%\n  #filter(mixture == tune_best$mixture) %>%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line(size = 1.5) +\n  scale_x_log10() +\n  theme(legend.position = \"none\") +\n  labs(title = \"RMSE\")\n\n\n\n\nshow_best(): mean값을 최소로 만들어주는 값들의 몇개를 보여준다\n\n\ncode\n\ntune_result %>% show_best()\n\n\n# A tibble: 5 x 7\n   penalty .metric .estimator  mean     n std_err .config             \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 3.85e- 2 rmse    standard   0.120     1      NA Preprocessor1_Model…\n2 4.86e- 2 rmse    standard   0.120     1      NA Preprocessor1_Model…\n3 1.00e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n4 1.26e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n5 1.59e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n\nSet Ridge regression model and fitting\nSet mixture is equal to zero refering the Ridge regression in glmnet since the\n위에서 구한 값으로 train2를 학습시킨다.\n\n\ncode\n\nelastic_model <- \n    linear_reg(penalty = tune_best$penalty, # 0.095\n               mixture = 0) %>%\n    set_engine(\"glmnet\")\nelastic_fit <- \n    elastic_model %>% \n    fit(sale_price ~ ., data = train2)\noptions(max.print = 10)\nelastic_fit %>% \n    tidy() %>% \n    filter(estimate > 0.001)\n\n\n# A tibble: 112 x 3\n   term           estimate penalty\n   <chr>             <dbl>   <dbl>\n 1 (Intercept)    12.0      0.0385\n 2 lot_frontage    0.0109   0.0385\n 3 lot_area        0.0177   0.0385\n 4 overall_qual    0.0553   0.0385\n 5 overall_cond    0.0358   0.0385\n 6 year_built      0.0292   0.0385\n 7 year_remod_add  0.0195   0.0385\n 8 mas_vnr_area    0.00380  0.0385\n 9 bsmt_fin_sf1    0.0269   0.0385\n10 bsmt_fin_sf2    0.00818  0.0385\n# … with 102 more rows\n\nPrediction\n\n\ncode\n\nresult <- predict(elastic_fit, test2)\nresult %>% head()\n\n\n# A tibble: 6 x 1\n  .pred\n  <dbl>\n1  11.7\n2  12.0\n3  12.1\n4  12.2\n5  12.2\n6  12.0\n\n\n\ncode\n\nX <- model.matrix(sale_price ~ ., train2)[, -1]\n\nY <- log(train2$sale_price)\n\n\n\n\n\ncode\n\n# Apply ridge regression to ames data\nridge <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\nplot(ridge, xvar = \"lambda\", main=\"Ridge\")\n\n\n\ncode\n\n# Lasso model\nlasso_min <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\nplot(ridge, xvar = \"lambda\", main=\"Lasso\")\n\n\n\ncode\n\n# Apply CV ridge regression to Ames data\nridge <- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\n# Apply CV lasso regression to Ames data\nlasso <- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\n# plot results\npar(mfrow = c(1, 2))\nplot(ridge, main = \"Ridge penalty\\n\\n\")\nplot(lasso, main = \"Lasso penalty\\n\\n\")\n\n\n\ncode\n\n# Ridge model\nridge_min <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\n# Lasso model\nlasso_min <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\npar(mfrow = c(1, 2))\n# plot ridge model\nplot(ridge_min, xvar = \"lambda\", main = \"Ridge penalty\\n\\n\")\nabline(v = log(ridge$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(ridge$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n# plot lasso model\nplot(lasso_min, xvar = \"lambda\", main = \"Lasso penalty\\n\\n\")\nabline(v = log(lasso$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(lasso$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n\n\n\n\n\ncode\n\n# Ridge model\nridge_min <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\n# Lasso model\nlasso_min <- glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\npar(mfrow = c(1, 2))\n\n# plot ridge model\nplot(ridge_min, xvar = \"lambda\", main = \"Ridge penalty\\n\\n\")\nabline(v = log(ridge$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(ridge$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n# plot lasso model\nplot(lasso_min, xvar = \"lambda\", main = \"Lasso penalty\\n\\n\")\nabline(v = log(lasso$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(lasso$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-15-hw7/distill-preview.png",
    "last_modified": "2021-05-28T07:31:01+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-10-fork-update/",
    "title": "fork update",
    "description": "local에 fork한 거 update하는 방법",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-04-10",
    "categories": [],
    "contents": "\n내가 fork한 곳에 교수님께서 update를 하셨는데,,\n내가 local로 작업하고 있는 곳ㅇ ㅔ 반영이 안되어서 방법을 찾다가 까먹기 전에 기록해두기…\n일단!\n아직 이 방식이 어떤 구조로 되는지는 완벽하게 파악하지 못함 ㅠㅠ\nterminal에 입력하시오!\n$ git remote -v\n> origin https://github.com/hanbyeol1322/predictivemodeling-with-r (fetch)\n> origin https://github.com/hanbyeol1322/predictivemodeling-with-r (push)\n$ git remote add update\n> https://github.com/issactoast/predictivemodeling-with-r\n$ git remote -v\n> origin https://github.com/hanbyeol1322/predictivemodeling-with-r (fetch)\n> origin https://github.com/hanbyeol1322/predictivemodeling-with-r (push)\n> update https://github.com/issactoast/predictivemodeling-with-r (fetch)\n> update https://github.com/issactoast/predictivemodeling-with-r (push)\n$ git fetch update\n$ git checkout main\n$ git merge update/main\n이렇게 하면 교수님께서 업데이트한 내용이 내 local에 반영됨 ~! ~!\n근데 문제가 있다..\n이렇게 하면 다른사람들 merge된 것까지 내 local에 다 따라오는데,,이 부분은 좀 더 찾아봐야겠다.\n알아볼 것: 교수님이 update한 거랑 내가 merge한 부분만 가져오는 방법?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-11T12:38:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-03-hw6/",
    "title": "전처리후 outlier 제거와 training",
    "description": "hw6",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-04-03",
    "categories": [],
    "contents": "\n2주 전에 전처리 하나만으로 점수가 엄청 오른 것을 확인 할 수 있었다. 이전 전처리 후 oulier의 제거와 학습을 통해 좀 더 예측력을 올려보자!!!!\n1. Outlier 제거\noutlier를 제거하기 위해 데이터를 다시 불러와서 시작한당 준비단계….\n1.1 Libraries\n\n\n\n1.2 Data load\n\n [1] \"_posts\"                \"_site.yml\"            \n [3] \"about.Rmd\"             \"blog_posts.Rmd\"       \n [5] \"docs\"                  \"index.Rmd\"            \n [7] \"Portfolio.Rmd\"         \"postcard.R\"           \n [9] \"project.Rproj\"         \"README.md\"            \n[11] \"sample_submission.csv\" \"test.csv\"             \n[13] \"tobi.jpg\"              \"train.csv\"            \n[15] \"프사.jpg\"             \n\n\n\n\n1.3 Outliers 확인하기\ngr_liv_area 변수에 있는 outlier를 확인해보자!\nAny thoughts about right 4 points?\n\n\n\n4500이상 되는 값들이 유독 튀는 것을 확인 할 수 있다. 4500 이상 되는 2개의 값을 필터 씌운 뒤에 다시 train data로 넣어보자 %<>%\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\"],[524,1299],[60,60],[\"RL\",\"RL\"],[130,313],[40094,63887],[\"Pave\",\"Pave\"],[null,null],[\"IR1\",\"IR3\"],[\"Bnk\",\"Bnk\"],[\"AllPub\",\"AllPub\"],[\"Inside\",\"Corner\"],[\"Gtl\",\"Gtl\"],[\"Edwards\",\"Edwards\"],[\"PosN\",\"Feedr\"],[\"PosN\",\"Norm\"],[\"1Fam\",\"1Fam\"],[\"2Story\",\"2Story\"],[10,10],[5,5],[2007,2008],[2008,2008],[\"Hip\",\"Hip\"],[\"CompShg\",\"ClyTile\"],[\"CemntBd\",\"Stucco\"],[\"CmentBd\",\"Stucco\"],[\"Stone\",\"Stone\"],[762,796],[\"Ex\",\"Ex\"],[\"TA\",\"TA\"],[\"PConc\",\"PConc\"],[\"Ex\",\"Ex\"],[\"TA\",\"TA\"],[\"Gd\",\"Gd\"],[\"GLQ\",\"GLQ\"],[2260,5644],[\"Unf\",\"Unf\"],[0,0],[878,466],[3138,6110],[\"GasA\",\"GasA\"],[\"Ex\",\"Ex\"],[\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\"],[3138,4692],[1538,950],[0,0],[4676,5642],[1,2],[0,0],[3,2],[1,1],[3,3],[1,1],[\"Ex\",\"Ex\"],[11,12],[\"Typ\",\"Typ\"],[1,3],[\"Gd\",\"Gd\"],[\"BuiltIn\",\"Attchd\"],[2007,2008],[\"Fin\",\"Fin\"],[3,2],[884,1418],[\"TA\",\"TA\"],[\"TA\",\"TA\"],[\"Y\",\"Y\"],[208,214],[406,292],[0,0],[0,0],[0,0],[0,480],[null,\"Gd\"],[null,null],[null,null],[0,0],[10,1],[2007,2008],[\"New\",\"New\"],[\"Partial\",\"Partial\"],[184750,160000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n\n\n\noutlier를 지우니 추세가 눈에 확 들어온다!\ntotal_bsmt_sf 변수에 있는 outlier를 확인해보자!\n\n\n\ntotal_bsmt_sf가 3000 이상 되는 값들이 튀는 것처럼 보인다.\noutlier점 3개중에 2개를 지웠다. (왜 로그값이 13보다 작은 값들을 지우셨는지는 좀 더 생각해보자)\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\"],[333,497],[20,20],[\"RL\",\"RL\"],[85,null],[10655,12692],[\"Pave\",\"Pave\"],[null,null],[\"IR1\",\"IR1\"],[\"Lvl\",\"Lvl\"],[\"AllPub\",\"AllPub\"],[\"Inside\",\"Inside\"],[\"Gtl\",\"Gtl\"],[\"NridgHt\",\"NoRidge\"],[\"Norm\",\"Norm\"],[\"Norm\",\"Norm\"],[\"1Fam\",\"1Fam\"],[\"1Story\",\"1Story\"],[8,8],[5,5],[2003,1992],[2004,1993],[\"Gable\",\"Hip\"],[\"CompShg\",\"CompShg\"],[\"VinylSd\",\"BrkFace\"],[\"VinylSd\",\"BrkFace\"],[\"BrkFace\",\"None\"],[296,0],[\"Gd\",\"Gd\"],[\"TA\",\"TA\"],[\"PConc\",\"PConc\"],[\"Gd\",\"Gd\"],[\"TA\",\"TA\"],[\"No\",\"No\"],[\"GLQ\",\"GLQ\"],[1124,1231],[null,\"Unf\"],[479,0],[1603,1969],[3206,3200],[\"GasA\",\"GasA\"],[\"Ex\",\"Ex\"],[\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\"],[1629,3228],[0,0],[0,0],[1629,3228],[1,1],[0,0],[2,3],[0,0],[3,4],[1,1],[\"Gd\",\"Gd\"],[7,10],[\"Typ\",\"Typ\"],[1,1],[\"Gd\",\"Gd\"],[\"Attchd\",\"Attchd\"],[2003,1992],[\"RFn\",\"RFn\"],[3,2],[880,546],[\"TA\",\"TA\"],[\"TA\",\"TA\"],[\"Y\",\"Y\"],[0,264],[0,75],[0,291],[0,0],[0,0],[0,0],[null,null],[null,null],[null,null],[0,0],[10,5],[2009,2007],[\"WD\",\"WD\"],[\"Normal\",\"Normal\"],[284000,430000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n\n\n\n잘 지워졌당~!\ngarage_area 변수에 있는 outlier를 확인해보자!\n\n\n\n1230보다 큰 3개의 점이 거슬린다.. 저걸 지워보쟈\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\"],[582,1062,1191],[20,30,190],[\"RL\",\"C (all)\",\"RL\"],[98,120,null],[12704,18000,32463],[\"Pave\",\"Grvl\",\"Pave\"],[null,null,null],[\"Reg\",\"Reg\",\"Reg\"],[\"Lvl\",\"Low\",\"Low\"],[\"AllPub\",\"AllPub\",\"AllPub\"],[\"Inside\",\"Inside\",\"Inside\"],[\"Gtl\",\"Gtl\",\"Mod\"],[\"NridgHt\",\"IDOTRR\",\"Mitchel\"],[\"Norm\",\"Norm\",\"Norm\"],[\"Norm\",\"Norm\",\"Norm\"],[\"1Fam\",\"1Fam\",\"2fmCon\"],[\"1Story\",\"1Story\",\"1Story\"],[8,3,4],[5,4,4],[2008,1935,1961],[2009,1950,1975],[\"Hip\",\"Gable\",\"Gable\"],[\"CompShg\",\"CompShg\",\"CompShg\"],[\"VinylSd\",\"MetalSd\",\"MetalSd\"],[\"VinylSd\",\"MetalSd\",\"MetalSd\"],[\"BrkFace\",\"None\",\"Stone\"],[306,0,149],[\"Ex\",\"Fa\",\"TA\"],[\"TA\",\"TA\",\"Gd\"],[\"PConc\",\"CBlock\",\"CBlock\"],[\"Ex\",\"TA\",\"TA\"],[\"TA\",\"TA\",\"TA\"],[\"No\",\"No\",\"Av\"],[\"Unf\",\"Unf\",\"BLQ\"],[0,0,1159],[\"Unf\",\"Unf\",\"Unf\"],[0,0,0],[2042,894,90],[2042,894,1249],[\"GasA\",\"GasA\",\"GasA\"],[\"Ex\",\"TA\",\"Ex\"],[\"Y\",\"Y\",\"Y\"],[\"SBrkr\",\"SBrkr\",\"SBrkr\"],[2042,894,1622],[0,0,0],[0,0,0],[2042,894,1622],[0,0,1],[0,0,0],[2,1,1],[1,0,0],[3,2,3],[1,1,1],[\"Ex\",\"TA\",\"TA\"],[8,6,7],[\"Typ\",\"Typ\",\"Typ\"],[1,0,1],[\"Gd\",null,\"TA\"],[\"Attchd\",\"Detchd\",\"2Types\"],[2009,1994,1975],[\"RFn\",\"RFn\",\"Fin\"],[3,3,4],[1390,1248,1356],[\"TA\",\"TA\",\"TA\"],[\"TA\",\"TA\",\"TA\"],[\"Y\",\"Y\",\"Y\"],[0,0,439],[90,20,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[null,null,null],[null,null,null],[null,\"Shed\",null],[0,560,0],[8,8,3],[2009,2008,2007],[\"New\",\"ConLD\",\"WD\"],[\"Partial\",\"Normal\",\"Normal\"],[253293,81000,168000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>ms_sub_class<\\/th>\\n      <th>ms_zoning<\\/th>\\n      <th>lot_frontage<\\/th>\\n      <th>lot_area<\\/th>\\n      <th>street<\\/th>\\n      <th>alley<\\/th>\\n      <th>lot_shape<\\/th>\\n      <th>land_contour<\\/th>\\n      <th>utilities<\\/th>\\n      <th>lot_config<\\/th>\\n      <th>land_slope<\\/th>\\n      <th>neighborhood<\\/th>\\n      <th>condition1<\\/th>\\n      <th>condition2<\\/th>\\n      <th>bldg_type<\\/th>\\n      <th>house_style<\\/th>\\n      <th>overall_qual<\\/th>\\n      <th>overall_cond<\\/th>\\n      <th>year_built<\\/th>\\n      <th>year_remod_add<\\/th>\\n      <th>roof_style<\\/th>\\n      <th>roof_matl<\\/th>\\n      <th>exterior1st<\\/th>\\n      <th>exterior2nd<\\/th>\\n      <th>mas_vnr_type<\\/th>\\n      <th>mas_vnr_area<\\/th>\\n      <th>exter_qual<\\/th>\\n      <th>exter_cond<\\/th>\\n      <th>foundation<\\/th>\\n      <th>bsmt_qual<\\/th>\\n      <th>bsmt_cond<\\/th>\\n      <th>bsmt_exposure<\\/th>\\n      <th>bsmt_fin_type1<\\/th>\\n      <th>bsmt_fin_sf1<\\/th>\\n      <th>bsmt_fin_type2<\\/th>\\n      <th>bsmt_fin_sf2<\\/th>\\n      <th>bsmt_unf_sf<\\/th>\\n      <th>total_bsmt_sf<\\/th>\\n      <th>heating<\\/th>\\n      <th>heating_qc<\\/th>\\n      <th>central_air<\\/th>\\n      <th>electrical<\\/th>\\n      <th>x1st_flr_sf<\\/th>\\n      <th>x2nd_flr_sf<\\/th>\\n      <th>low_qual_fin_sf<\\/th>\\n      <th>gr_liv_area<\\/th>\\n      <th>bsmt_full_bath<\\/th>\\n      <th>bsmt_half_bath<\\/th>\\n      <th>full_bath<\\/th>\\n      <th>half_bath<\\/th>\\n      <th>bedroom_abv_gr<\\/th>\\n      <th>kitchen_abv_gr<\\/th>\\n      <th>kitchen_qual<\\/th>\\n      <th>tot_rms_abv_grd<\\/th>\\n      <th>functional<\\/th>\\n      <th>fireplaces<\\/th>\\n      <th>fireplace_qu<\\/th>\\n      <th>garage_type<\\/th>\\n      <th>garage_yr_blt<\\/th>\\n      <th>garage_finish<\\/th>\\n      <th>garage_cars<\\/th>\\n      <th>garage_area<\\/th>\\n      <th>garage_qual<\\/th>\\n      <th>garage_cond<\\/th>\\n      <th>paved_drive<\\/th>\\n      <th>wood_deck_sf<\\/th>\\n      <th>open_porch_sf<\\/th>\\n      <th>enclosed_porch<\\/th>\\n      <th>x3ssn_porch<\\/th>\\n      <th>screen_porch<\\/th>\\n      <th>pool_area<\\/th>\\n      <th>pool_qc<\\/th>\\n      <th>fence<\\/th>\\n      <th>misc_feature<\\/th>\\n      <th>misc_val<\\/th>\\n      <th>mo_sold<\\/th>\\n      <th>yr_sold<\\/th>\\n      <th>sale_type<\\/th>\\n      <th>sale_condition<\\/th>\\n      <th>sale_price<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,4,5,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n\n\n\n잘 지워졌구나 ~!\n2. training\n이제 거슬리는 outliers을 지웠으니 학습을 통해 예측력을 높여보자 !!!\n1번에서 outliers를 제거한 train set을 이용해서 training 해볼 것이다.!\n먼저 outlier를 제거한 train과 test를 다시 전처리해준다.\n\n [1] \"id\"           \"ms_sub_class\" \"ms_zoning\"    \"lot_frontage\"\n [5] \"lot_area\"     \"street\"       \"alley\"        \"lot_shape\"   \n [9] \"land_contour\" \"utilities\"   \n\nMake recipe\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2912 data points and 2912 incomplete rows. \n\nOperations:\n\nVariables removed id [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, alley, ... [trained]\nDummy variables from ms_zoning, street, alley, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\njuice the all_data2 and split\n\n\n\nWe are done for preprocessing. Let’s split the data set.\n\n\n\n전처리가 다 끝났으면 이제 어떻게 학습할 것인지 알아보자\n2.1 Elastic net\ntrain set을 validation set으로 다시 분리 시킬 것이다. 70%는 학습용으로 남겨놓아라. 30%는 평가용!!\n\n\n\n이것이 실제로 어떻게 나눠져 있는지 확인하려면\nin_id : 학습 할 때 사용하는 것 out_id : 평가 할 때 사용하는 것\n\n   [1]    1    2    3    4    5    6    9   11   13   14   15   16\n  [13]   18   19   21   22   23   24   25   26   28   30   31   32\n  [25]   33   34   35   36   37   38   39   40   42   45   46   50\n  [37]   51   52   54   56   58   61   62   64   65   66   67   68\n  [49]   69   71   72   73   74   78   80   82   83   85   86   87\n  [61]   88   90   91   92   94   96   97   99  100  102  104  105\n  [73]  106  108  109  110  111  112  114  115  117  119  120  121\n  [85]  122  124  125  126  128  130  131  132  133  135  136  137\n  [97]  138  140  141  142  143  144  145  146  147  149  150  151\n [109]  152  153  157  158  159  160  161  162  163  165  168  169\n [121]  170  171  173  176  180  181  182  183  184  186  188  189\n [133]  190  193  194  195  196  197  200  201  202  203  204  206\n [145]  207  208  209  210  211  212  213  214  215  216  217  218\n [157]  219  220  221  222  223  224  225  228  229  231  233  234\n [169]  236  237  238  239  242  243  244  245  247  248  249  250\n [181]  252  253  255  256  257  259  260  261  263  264  265  266\n [193]  267  268  269  270  271  272  273  274  277  280  281  283\n [205]  284  286  287  288  289  290  291  292  293  296  297  298\n [217]  299  301  303  304  308  309  310  311  314  315  316  318\n [229]  319  322  323  324  326  328  329  330  331  332  333  334\n [241]  335  336  337  338  343  345  346  347  348  349  352  353\n [253]  354  355  356  357  358  359  363  364  366  367  368  370\n [265]  371  372  375  376  377  378  379  380  381  382  383  384\n [277]  386  389  391  393  394  397  398  399  401  404  405  407\n [289]  408  409  410  411  412  413  414  415  416  417  418  419\n [301]  421  422  423  424  425  426  428  429  431  433  434  435\n [313]  437  438  439  440  441  443  444  447  448  450  452  455\n [325]  456  457  459  460  462  463  464  466  467  468  469  470\n [337]  473  474  476  477  478  479  480  482  483  484  486  487\n [349]  488  490  493  494  496  497  498  499  500  501  502  505\n [361]  506  507  508  509  512  514  515  516  517  518  519  520\n [373]  521  524  525  526  527  528  529  530  531  532  533  534\n [385]  536  537  539  540  541  542  544  545  546  547  548  549\n [397]  552  553  554  555  556  557  558  559  560  561  562  563\n [409]  564  565  566  567  568  570  571  572  573  574  575  576\n [421]  578  579  582  584  585  586  587  588  589  590  591  592\n [433]  593  594  595  596  597  598  603  604  605  610  612  613\n [445]  614  615  617  618  619  620  621  623  626  627  629  630\n [457]  631  633  634  635  636  638  640  641  643  644  646  647\n [469]  648  650  652  654  656  657  658  659  662  663  665  668\n [481]  672  675  676  677  679  684  685  688  689  690  691  692\n [493]  693  694  695  696  697  698  699  700  701  702  704  705\n [505]  706  707  708  709  711  715  717  718  719  721  723  725\n [517]  727  730  731  733  734  735  736  738  740  741  744  745\n [529]  746  747  748  749  750  751  752  753  755  756  757  759\n [541]  760  761  763  765  766  767  770  771  772  773  774  775\n [553]  776  777  779  780  781  783  786  787  788  789  791  792\n [565]  794  795  796  798  799  800  802  803  804  805  807  810\n [577]  811  813  815  816  817  818  819  820  822  823  824  825\n [589]  826  828  829  830  832  834  835  837  838  839  840  842\n [601]  845  846  847  848  849  850  851  852  853  854  856  857\n [613]  858  859  860  861  862  863  864  865  867  868  869  871\n [625]  872  874  875  876  877  879  880  882  885  888  889  890\n [637]  891  892  895  896  897  898  901  902  905  906  910  912\n [649]  913  914  916  917  918  920  921  922  923  924  926  927\n [661]  929  930  931  935  936  937  938  940  941  942  943  948\n [673]  949  950  951  952  953  954  955  957  958  959  960  962\n [685]  964  965  966  968  969  971  972  975  976  977  978  981\n [697]  983  984  985  987  988  991  993  994  995  996  997  998\n [709]  999 1001 1002 1003 1005 1008 1010 1011 1013 1014 1016 1017\n [721] 1018 1020 1021 1022 1024 1026 1027 1029 1030 1031 1033 1034\n [733] 1035 1038 1039 1040 1041 1042 1043 1045 1047 1048 1049 1050\n [745] 1053 1054 1055 1056 1057 1059 1060 1061 1062 1063 1064 1066\n [757] 1067 1068 1069 1070 1071 1072 1073 1075 1076 1077 1079 1080\n [769] 1081 1082 1083 1086 1088 1089 1090 1092 1095 1096 1098 1099\n [781] 1100 1101 1102 1103 1104 1105 1107 1108 1110 1111 1112 1113\n [793] 1117 1119 1121 1123 1124 1125 1126 1127 1128 1129 1131 1134\n [805] 1135 1136 1137 1138 1140 1141 1142 1143 1146 1151 1152 1154\n [817] 1155 1156 1158 1159 1160 1161 1162 1163 1165 1169 1171 1173\n [829] 1177 1180 1181 1183 1184 1185 1186 1187 1188 1190 1191 1193\n [841] 1194 1196 1198 1199 1200 1201 1202 1203 1205 1207 1209 1210\n [853] 1211 1213 1214 1216 1217 1218 1220 1223 1224 1225 1226 1228\n [865] 1229 1230 1231 1232 1233 1234 1236 1240 1241 1242 1243 1244\n [877] 1245 1247 1248 1249 1250 1251 1252 1254 1255 1256 1262 1263\n [889] 1265 1266 1267 1268 1269 1270 1272 1273 1275 1276 1278 1279\n [901] 1280 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1294\n [913] 1295 1297 1300 1304 1306 1307 1308 1309 1311 1312 1314 1315\n [925] 1316 1320 1321 1322 1324 1327 1328 1329 1330 1331 1333 1334\n [937] 1335 1337 1338 1339 1340 1341 1342 1343 1346 1347 1349 1350\n [949] 1351 1352 1354 1355 1357 1360 1361 1362 1363 1365 1368 1369\n [961] 1370 1371 1372 1374 1375 1376 1377 1379 1383 1384 1386 1388\n [973] 1389 1390 1391 1392 1394 1395 1398 1399 1400 1401 1402 1404\n [985] 1405 1406 1407 1409 1410 1411 1413 1414 1415 1416 1421 1422\n [997] 1423 1424 1425 1428 1429 1430 1432 1433 1434 1435 1436 1437\n[1009] 1438 1440 1441 1442 1446 1448 1450 1451 1452 1453\n[1] 1 2 3 4 5 6\n\n2.1.1 Ridge Regression\n이제 tune을 해야하는데 Elastic net에서 \\(\\alpha\\)(=mixture),penalty(=lambda)\nridge regression: Mixture=0\npenalty는 아직 안정했음 : tune 할 것이다.\nSet the tuning spec 람다의 후보군을 여러개 남기기 위해서\ntune_spec: ridge regression에 penalty는 아직 정해지지 않은 상태\nglmnet: elastic net을 사용하기 위한 packaages\nparam_grid : 후보가 될 수 있는 람다의 값이 들어있음(하나하나 학습 시킬 것)0이 없고, 1이 무한대인 상태!\ngrid_regular(levels = 50) : 0에서 1까지 중에 균등하게 50개를 뽑은 것\n\n\n\nSet workflow()\n평가하기 위한 모델을 만들기\n\n\n\nTuning \\(\\lambda\\) and \\(\\alpha\\) validation_split(학습, test데이터)에다가 param_grid를 fitting한다.\n어떤 것이 좋은지 평가하는 방식은 rmse(soot mean square error)를 이용하겠다.\n-> tune_result에 각 람다 값에 대한 퍼포먼스가 담기게 된다.\n\n0.876 sec elapsed\n\ncollect_metrics(): 퍼포먼스가 담긴 것들을 모아봐라\nmean 값: rmse값(validation에서 나온 것 들의 평균)\n우리의 목표: mean 값이 최소가 되는 penalty값을 구하는 것\n\n# A tibble: 50 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 2 1.60e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 3 2.56e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 4 4.09e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 5 6.55e-10 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 6 1.05e- 9 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 7 1.68e- 9 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 8 2.68e- 9 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n 9 4.29e- 9 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n10 6.87e- 9 rmse    standard   0.120     1      NA Preprocessor1_Mode…\n# … with 40 more rows\n\nVisualization of the tunning result\nmixture은 지금 없으니까(ridge) 잠시 나가있어\n\n[1] 0.03727594\n\ntune_result안네 penalty대비 rmse의 값을 보여주는 plot\n\n\n\nshow_best(): mean값을 최소로 만들어주는 값들의 몇개를 보여준다\n\n# A tibble: 5 x 7\n   penalty .metric .estimator  mean     n std_err .config             \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 3.73e- 2 rmse    standard   0.120     1      NA Preprocessor1_Model…\n2 1.00e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n3 1.60e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n4 2.56e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n5 4.09e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n\nSet Ridge regression model and fitting\nSet mixture is equal to zero refering the Ridge regression in glmnet since the\n위에서 구한 값으로 train2를 학습시킨다.\n\n# A tibble: 111 x 3\n   term           estimate penalty\n   <chr>             <dbl>   <dbl>\n 1 (Intercept)    12.0      0.0373\n 2 lot_frontage    0.0109   0.0373\n 3 lot_area        0.0178   0.0373\n 4 overall_qual    0.0554   0.0373\n 5 overall_cond    0.0360   0.0373\n 6 year_built      0.0296   0.0373\n 7 year_remod_add  0.0195   0.0373\n 8 mas_vnr_area    0.00369  0.0373\n 9 bsmt_fin_sf1    0.0270   0.0373\n10 bsmt_fin_sf2    0.00824  0.0373\n# … with 101 more rows\n\nPrediction and submit\n\n# A tibble: 6 x 1\n  .pred\n  <dbl>\n1  11.7\n2  12.0\n3  12.1\n4  12.2\n5  12.2\n6  12.0\n\n\n\n\n오늘 배운 것은 train을 둘로 쪼개서 하나는 가상의 test set으로 하고 하나는 train으로\n평가하는 validation set이 test set과 비슷하면 좋겟지만, 비슷하지 못할 경우를 대비해서 cross validation을 진행한다.!!\n방법은 validation set을 여러개로 만드는 것인ㄷㅔ,, v=10: train data를 10개로 나누되, saleprice가 골고루 나눠서 담길 수 있도록 하라.\n\n\n\n알파와 람다를 정하지 않고 tune을 하라. 두가지를 고르는 것이니까 mixture도 tune으로 해주기\n\n\n\n각 fold에 대해서 가능한 후보군 집합이 있다. rmse말고 다른 (mae)것도 함 께 볼 수 있다 .\n에러남\n\n\n\n\n[1] 0.03727594\nNULL\n\n결과에 mixture=0이 나오면 regression방법으로 lasso 보다 ridge를 사용하는 것이 예측력에 더 좋다는 것을 확인 할 수 있음\n에러\n\n\n\nshow_best(): mean값을 최소로 만들어주는 값들의 몇개를 보여준다\n\n# A tibble: 5 x 7\n   penalty .metric .estimator  mean     n std_err .config             \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 3.73e- 2 rmse    standard   0.120     1      NA Preprocessor1_Model…\n2 1.00e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n3 1.60e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n4 2.56e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n5 4.09e-10 rmse    standard   0.120     1      NA Preprocessor1_Model…\n\nSet Ridge regression model and fitting\nSet mixture is equal to zero refering the Ridge regression in glmnet since the 위에서 구한 값으로 train2를 학습시킨다.\n에러\n\n# A tibble: 111 x 3\n   term           estimate penalty\n   <chr>             <dbl>   <dbl>\n 1 (Intercept)    12.0      0.0373\n 2 lot_frontage    0.0109   0.0373\n 3 lot_area        0.0178   0.0373\n 4 overall_qual    0.0554   0.0373\n 5 overall_cond    0.0360   0.0373\n 6 year_built      0.0296   0.0373\n 7 year_remod_add  0.0195   0.0373\n 8 mas_vnr_area    0.00369  0.0373\n 9 bsmt_fin_sf1    0.0270   0.0373\n10 bsmt_fin_sf2    0.00824  0.0373\n# … with 101 more rows\n\nPrediction and submit\n\n# A tibble: 6 x 1\n  .pred\n  <dbl>\n1  11.7\n2  12.0\n3  12.1\n4  12.2\n5  12.2\n6  12.0\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-03-hw6/hw6_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-11T10:40:36+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-01-lec5/",
    "title": "회귀선의 계수를 구하는 방법",
    "description": "lec5",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-04-01",
    "categories": [],
    "contents": "\n\nContents\n1. beta\n2. mtcars data를 이용해서 직접 구해보기\n2.1 X와 y 값 설정\n2.2 intercept 열 추가\n2.3 행렬로 구한 값과 실제 선형회귀 결과 값을 비교\n2.4 위 데이터를 다항식을 이용해서 표현하기\n\n2.5 행렬을 이용해서 3차 회귀계수방정식, beta구하기\n\n이번 시간에는 회귀선의 계수를 구하는 방법을 이론적으로 풀어보고, 실제로 행렬을 이용해서 구해봤다.\n1. beta\n알고있는 점과 회귀선의 직선거리가 최소가 되게 하는 계수를 beta라고 했을 때 행렬을 이용해서 풀면 다음과 같다.\n\\[beta = (X^TX)^{-1}X^Ty \\]\n2. mtcars data를 이용해서 직접 구해보기\n\n\ncode\n\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n2.1 X와 y 값 설정\ny 값으로는 mpg, X 벡터들로는 cyl,disp, hp를 가져온다.\n\n\ncode\n\ny <- as.vector(mtcars[,1]) \nX <- as.matrix(mtcars[,2:4])\n\n\n\n2.2 intercept 열 추가\nintercept를 추가해주기 위해 cbind()를 이용해서 열을 더해준다.\n\n\ncode\n\nX <- cbind(1,X)\n\n\n\nTranspose: t(X)\n행렬의 곱: %*%\n역행렬: solve()\n따라서 우리가 구하고자 하는 식을 행렬로 표현하면\n\n\ncode\n\nbeta <- solve(t(X) %*% X) %*% t(X) %*% y\n\n\n\n2.3 행렬로 구한 값과 실제 선형회귀 결과 값을 비교\n행렬을 이용해서 구한 beta가 실제 regression을 했을 때 나오는 beta랑 같은지 확인\n\n\ncode\n\nresult <- lm(mpg ~ cyl + disp + hp, data=mtcars) \nresult$coefficients\n\n\n(Intercept)         cyl        disp          hp \n34.18491917 -1.22741994 -0.01883809 -0.01467933 \n\ncode\n\nlibrary(tibble) \nset.seed(2021)\n\n\n\nx: 0에서 1사이에 0.05씩 떨어져 있는 수 중에서 10개를 무작위로 뽑아라\n\n\ncode\n\nx <- sample(seq(0,1,by=0.05), 10) \n\n\n\ny: 그 x를 sin 2pi에 넣고 epsilon(rnorm, sd=0.2인)을 더해라\n\n\ncode\n\ny <- sin(2* pi * x) + rnorm(length(x), sd=0.2)\n\n\n\n\n\ncode\n\nmydata <- tibble(x = x, y = y) \nmydata\n\n\n# A tibble: 10 x 2\n       x      y\n   <dbl>  <dbl>\n 1  0.3   1.00 \n 2  0.25  1.18 \n 3  0.65 -0.806\n 4  1     0.346\n 5  0.55 -0.525\n 6  0.15  0.754\n 7  0.95 -0.273\n 8  0.7  -0.649\n 9  0.5   0.321\n10  0.9  -0.956\n\n원래 값과 비교\n\n\ncode\n\nx2 <- seq(0, 1, by = 0.01) \ny2 <- sin(2 * pi * x2) \nplot(x, y) \npoints(x2, y2, type=\"l\", col = \"red\")\n\n\n\n\n2.4 위 데이터를 다항식을 이용해서 표현하기\n0차 polynomial\n\n\ncode\n\nx2 <- seq(0, 1, by = 0.01) \ny2 <- sin(2 * pi * x2) \nplot(x,y) \npoints(x2, y2, type=\"l\", col = \"red\") \nabline(h = mean(y), col = \"blue\")\n\n\n\n\n1차식으로 회귀직선 구하기\n\n\ncode\n\nx2 <- seq(0, 1, by = 0.01) \ny2 <- sin(2 * pi * x2) \nresult <- lm(y~x, data = mydata) \nplot(x, y) \npoints(x2, y2, type=\"l\", col = \"red\") \nabline(result, col=\"blue\")\n\n\n\n\n3차식으로 회귀직선 구하기\n\n\ncode\n\nx2 <- seq(0, 1, by = 0.01) \ny2 <- sin(2* pi * x2)\nresult <- lm(y ~ poly(x, 3), data = mydata)\nplot(x, y) \npoints(x2, y2, type =\"l\", col = \"red\") \ny3 <- predict(result, newdata = tibble(x = x2)) \npoints(x2, y3, type=\"l\", col=\"blue\")\n\n\n\n\n2.5 행렬을 이용해서 3차 회귀계수방정식, beta구하기\n\n\ncode\n\nX <- cbind(1, x, x^2, x^3) \nbeta <- solve(t(X) %*% X) %*% t(X) %*% y\n\n\n\n구한 베타로 예측하는 곡선 그리기\n\n\ncode\n\nbeta <- as.vector(beta) \nx3 <- seq(0, 1, by=0.01)\n\ny_hat <- beta[1] + x3 * beta[2] + x3^2 * beta[3] + x3^3 * beta[4]\nplot(x,y) \npoints(x2, y2, type = \"l\", col = \"red\") \npoints(x3, y_hat, type = \"l\", col = \"blue\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-01-lec5/distill-preview.png",
    "last_modified": "2021-04-01T13:56:08+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-25-hw4/",
    "title": "saleprice prediction 수정(tidymodels)",
    "description": "응용통계학 hw4",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-03-25",
    "categories": [],
    "contents": "\n\nContents\n1. Preparations (준비작업)\n1.1 Libraries\n1.2 Data load\n\n2. Data overview (데이터 기본정보)\n2.1 Basic info.\n2.2 Detailed info. train\n2.3 Detailed info. test\n\n3. EDA with visualization (탐색적 데이터 분석)\n3.1 Distribution of sale_price\n3.2 NAs\n\n4. Preprecessing with recipe (전처리 레시피 만들기)\n4.1 all_data combine and name cleaning with janitor\n4.2 Make recipe\n4.3 juice the all_data2 and split\n\n5. Set linear regression model and fitting (모델 설정 및 학습)\n6. Prediction and submit (예측 및 평가)\n6.1 회귀분석 결과\n\n\n1. Preparations (준비작업)\n1.1 Libraries\nWe mainly exploits the functions from the tidyvers and tidymodels packages. magrittr has my favorite operators!\n\n\ncode\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(skimr)\nlibrary(knitr)\ntheme_set(theme_bw())\n\n\n\n1.2 Data load\nlist.files(): 지금 있는 파일이 뭐니?\n\n\ncode\n\nfile_path <- \"/cloud/project\"\nfiles <- list.files(file_path)\nfiles\n\n\n [1] \"_posts\"                \"_site.yml\"            \n [3] \"about.Rmd\"             \"blog_posts.Rmd\"       \n [5] \"docs\"                  \"index.Rmd\"            \n [7] \"Portfolio.Rmd\"         \"postcard.R\"           \n [9] \"project.Rproj\"         \"README.md\"            \n[11] \"sample_submission.csv\" \"test.csv\"             \n[13] \"tobi.jpg\"              \"train.csv\"            \n[15] \"프사.jpg\"             \n\ncsv파일 불러오기\n\n\ncode\n\ntrain <- read_csv(file.path(file_path, \"train.csv\"))\ntest <- read_csv(file.path(file_path, \"test.csv\"))\n\n\n\n2. Data overview (데이터 기본정보)\n2.1 Basic info.\nHere is the basic information about train and test. We have approximately the same sample size for the train and test set. The number of columns in the train is 81 and the one in the test is 80.\n\n\ncode\n\ndim(train)\n\n\n[1] 1460   81\n\ncode\n\ndim(test)\n\n\n[1] 1459   80\n\nWe can see train doesn’t have the target variable SalePrice.\n\n\ncode\n\n\"SalePrice\" %in% names(test)\n\n\n[1] FALSE\n\n2.2 Detailed info. train\n\n\ncode\n\nskim(train)\n\n\nTable 1: Data summary\nName\ntrain\nNumber of rows\n1460\nNumber of columns\n81\n_______________________\n\nColumn type frequency:\n\ncharacter\n43\nnumeric\n38\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\nStreet\n0\n1.00\n4\n4\n0\n2\n0\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\nHeating\n0\n1.00\n4\n5\n0\n6\n0\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\nFence\n1179\n0.19\n4\n5\n0\n4\n0\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n2.3 Detailed info. test\n\n\ncode\n\nskim(test)\n\n\nTable 2: Data summary\nName\ntest\nNumber of rows\n1459\nNumber of columns\n80\n_______________________\n\nColumn type frequency:\n\ncharacter\n43\nnumeric\n37\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nMSZoning\n4\n1.00\n2\n7\n0\n5\n0\nStreet\n0\n1.00\n4\n4\n0\n2\n0\nAlley\n1352\n0.07\n4\n4\n0\n2\n0\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\nUtilities\n2\n1.00\n6\n6\n0\n1\n0\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\nCondition2\n0\n1.00\n4\n6\n0\n5\n0\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\nHouseStyle\n0\n1.00\n4\n6\n0\n7\n0\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\nRoofMatl\n0\n1.00\n7\n7\n0\n4\n0\nExterior1st\n1\n1.00\n6\n7\n0\n13\n0\nExterior2nd\n1\n1.00\n5\n7\n0\n15\n0\nMasVnrType\n16\n0.99\n4\n7\n0\n4\n0\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\nBsmtQual\n44\n0.97\n2\n2\n0\n4\n0\nBsmtCond\n45\n0.97\n2\n2\n0\n4\n0\nBsmtExposure\n44\n0.97\n2\n2\n0\n4\n0\nBsmtFinType1\n42\n0.97\n3\n3\n0\n6\n0\nBsmtFinType2\n42\n0.97\n3\n3\n0\n6\n0\nHeating\n0\n1.00\n4\n4\n0\n4\n0\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\nElectrical\n0\n1.00\n5\n5\n0\n4\n0\nKitchenQual\n1\n1.00\n2\n2\n0\n4\n0\nFunctional\n2\n1.00\n3\n4\n0\n7\n0\nFireplaceQu\n730\n0.50\n2\n2\n0\n5\n0\nGarageType\n76\n0.95\n6\n7\n0\n6\n0\nGarageFinish\n78\n0.95\n3\n3\n0\n3\n0\nGarageQual\n78\n0.95\n2\n2\n0\n4\n0\nGarageCond\n78\n0.95\n2\n2\n0\n5\n0\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\nPoolQC\n1456\n0.00\n2\n2\n0\n2\n0\nFence\n1169\n0.20\n4\n5\n0\n4\n0\nMiscFeature\n1408\n0.03\n4\n4\n0\n3\n0\nSaleType\n1\n1.00\n2\n5\n0\n9\n0\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nId\n0\n1.00\n2190.00\n421.32\n1461\n1825.50\n2190.0\n2554.50\n2919\n▇▇▇▇▇\nMSSubClass\n0\n1.00\n57.38\n42.75\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\nLotFrontage\n227\n0.84\n68.58\n22.38\n21\n58.00\n67.0\n80.00\n200\n▃▇▁▁▁\nLotArea\n0\n1.00\n9819.16\n4955.52\n1470\n7391.00\n9399.0\n11517.50\n56600\n▇▂▁▁▁\nOverallQual\n0\n1.00\n6.08\n1.44\n1\n5.00\n6.0\n7.00\n10\n▁▁▇▅▁\nOverallCond\n0\n1.00\n5.55\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\nYearBuilt\n0\n1.00\n1971.36\n30.39\n1879\n1953.00\n1973.0\n2001.00\n2010\n▁▂▃▆▇\nYearRemodAdd\n0\n1.00\n1983.66\n21.13\n1950\n1963.00\n1992.0\n2004.00\n2010\n▅▂▂▃▇\nMasVnrArea\n15\n0.99\n100.71\n177.63\n0\n0.00\n0.0\n164.00\n1290\n▇▁▁▁▁\nBsmtFinSF1\n1\n1.00\n439.20\n455.27\n0\n0.00\n350.5\n753.50\n4010\n▇▂▁▁▁\nBsmtFinSF2\n1\n1.00\n52.62\n176.75\n0\n0.00\n0.0\n0.00\n1526\n▇▁▁▁▁\nBsmtUnfSF\n1\n1.00\n554.29\n437.26\n0\n219.25\n460.0\n797.75\n2140\n▇▆▂▁▁\nTotalBsmtSF\n1\n1.00\n1046.12\n442.90\n0\n784.00\n988.0\n1305.00\n5095\n▇▇▁▁▁\n1stFlrSF\n0\n1.00\n1156.53\n398.17\n407\n873.50\n1079.0\n1382.50\n5095\n▇▃▁▁▁\n2ndFlrSF\n0\n1.00\n325.97\n420.61\n0\n0.00\n0.0\n676.00\n1862\n▇▃▂▁▁\nLowQualFinSF\n0\n1.00\n3.54\n44.04\n0\n0.00\n0.0\n0.00\n1064\n▇▁▁▁▁\nGrLivArea\n0\n1.00\n1486.05\n485.57\n407\n1117.50\n1432.0\n1721.00\n5095\n▇▇▁▁▁\nBsmtFullBath\n2\n1.00\n0.43\n0.53\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\nBsmtHalfBath\n2\n1.00\n0.07\n0.25\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\nFullBath\n0\n1.00\n1.57\n0.56\n0\n1.00\n2.0\n2.00\n4\n▁▇▇▁▁\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\nBedroomAbvGr\n0\n1.00\n2.85\n0.83\n0\n2.00\n3.0\n3.00\n6\n▁▃▇▂▁\nKitchenAbvGr\n0\n1.00\n1.04\n0.21\n0\n1.00\n1.0\n1.00\n2\n▁▁▇▁▁\nTotRmsAbvGrd\n0\n1.00\n6.39\n1.51\n3\n5.00\n6.0\n7.00\n15\n▅▇▃▁▁\nFireplaces\n0\n1.00\n0.58\n0.65\n0\n0.00\n0.0\n1.00\n4\n▇▇▁▁▁\nGarageYrBlt\n78\n0.95\n1977.72\n26.43\n1895\n1959.00\n1979.0\n2002.00\n2207\n▂▇▁▁▁\nGarageCars\n1\n1.00\n1.77\n0.78\n0\n1.00\n2.0\n2.00\n5\n▅▇▂▁▁\nGarageArea\n1\n1.00\n472.77\n217.05\n0\n318.00\n480.0\n576.00\n1488\n▃▇▃▁▁\nWoodDeckSF\n0\n1.00\n93.17\n127.74\n0\n0.00\n0.0\n168.00\n1424\n▇▁▁▁▁\nOpenPorchSF\n0\n1.00\n48.31\n68.88\n0\n0.00\n28.0\n72.00\n742\n▇▁▁▁▁\nEnclosedPorch\n0\n1.00\n24.24\n67.23\n0\n0.00\n0.0\n0.00\n1012\n▇▁▁▁▁\n3SsnPorch\n0\n1.00\n1.79\n20.21\n0\n0.00\n0.0\n0.00\n360\n▇▁▁▁▁\nScreenPorch\n0\n1.00\n17.06\n56.61\n0\n0.00\n0.0\n0.00\n576\n▇▁▁▁▁\nPoolArea\n0\n1.00\n1.74\n30.49\n0\n0.00\n0.0\n0.00\n800\n▇▁▁▁▁\nMiscVal\n0\n1.00\n58.17\n630.81\n0\n0.00\n0.0\n0.00\n17000\n▇▁▁▁▁\nMoSold\n0\n1.00\n6.10\n2.72\n1\n4.00\n6.0\n8.00\n12\n▅▆▇▃▃\nYrSold\n0\n1.00\n2007.77\n1.30\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▃\n\n3. EDA with visualization (탐색적 데이터 분석)\n3.1 Distribution of sale_price\nIf we check out the distribution of the house price, it is little bit skewed to the right.\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = SalePrice)) +\n  geom_histogram()\n\n\n\n\nSince we want to build a linear regression assume that the noise follows the normal distribution, let us take a log to SalePrice variable.\n한쪽으로 치우쳐져 있는 값에 log를 취한다.(y가 정규분포를 따른다는 가정하에서 회귀분석이 잘 된ㄷㅣ.)\n\n\ncode\n\ntrain %>% \n  ggplot(aes(x = log(SalePrice))) +\n  geom_histogram()\n\n\n\n\n3.2 NAs\nThere is a nice package for checking out NAs. Let’s see how many variables we have which contains NAs. in.na인 값들을 다 더하면-> 그 col안에 missing observation이 있는 값들을 선택해서 gg_miss_var 함수로표현\n\n\ncode\n\nlibrary(naniar)\ntrain %>% \n  # select_if(~sum(is.na(.)) > 0) %>% # alternative way\n  select(where(~sum(is.na(.)) > 0)) %>% \n  gg_miss_var()\n\n\n\n\nWe can do more analysis about NAs with upset() function, which shows that most of the observations with NAs in the data set have NAs at the PoolQC, MiscFeature, Alley, Fence at the same time. 동시에없는 정보들을 볼 수 있다. (NA의 연관성으로 전처리를 할 때 고려할 수있다.)\n\n\ncode\n\ntrain %>% \n  select(where(~sum(is.na(.)) > 0)) %>% \n  gg_miss_upset()\n\n\n\n\nFrom the above, we can have some insights that if a house doesn’t have Pool, it is likely that it doesn’t have Alley, Fence, and Fireplace too.\n4. Preprecessing with recipe (전처리 레시피 만들기)\nFirst, I would like to clean the variable names with janitor package so that we have consistent varible names.\n4.1 all_data combine and name cleaning with janitor\nbind_rows하는 이유: 두 자료에 동일한 전처리를 하기 위해서\n\n\ncode\n\nall_data <- bind_rows(train, test) %>% \n  janitor::clean_names()\nnames(all_data)[1:10]\n\n\n [1] \"id\"           \"ms_sub_class\" \"ms_zoning\"    \"lot_frontage\"\n [5] \"lot_area\"     \"street\"       \"alley\"        \"lot_shape\"   \n [9] \"land_contour\" \"utilities\"   \n\ncode\n\nall_data %>% dim()\n\n\n[1] 2919   81\n\n4.2 Make recipe\nNote that we will use mode imputation for nominal variables for the baseline, and the mean imputation for the numerical variables. However, this should be changed to build a more sensitive model because we have checked that the NA in the nominal variables indicates that cases where the house doesn’t have the corresponding attributes.\n\n\ncode\n\nhousing_recipe <- all_data %>% \n  recipe(sale_price ~ .) %>%\n  step_rm(id,pool_qc, misc_feature,alley,fence, fireplace_qu) %>% \n  step_log(sale_price) %>% \n  step_modeimpute(all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_meanimpute(all_predictors()) %>%\n  step_normalize(all_predictors()) %>% \n  prep(training = all_data)\n\nprint(housing_recipe)\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2919 data points and 2919 incomplete rows. \n\nOperations:\n\nVariables removed id, pool_qc, misc_feature, alley, ... [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, ... [trained]\nDummy variables from ms_zoning, street, lot_shape, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\nall_data로 모델링을 할거다\nrecipe(): target variable은 saleprice이고,나머지 변수들은 예측변수로 쓸거다\nstep_rm(): id는 제거를 해라(별 필요 없을 거 같아서)\nstep_log():saleprice에 log를 씌워라\nstep_modeimpute(): na를 채우는 방법(impute)은 가장 빈번하게 있는 것으로(mode) 채워라. norminal애들만(숫자들이 아닌거)\nstep_dummy(): (tidymodels 패키지에서는)카테고리 데이터에 더미코딩을 알아서 해줘(base: data.metrirx)\nstep_meanimputae():numerical애들은 그 na 제외한 col의 평균을 구해서 채워 넣어라\nstep_normalize(): 예측값을 정규화시킴: col들의 평균, 분산을 구해서 표준화를 시킨다.\nprep(): 위의 전처리 과정을 거친 all_data를 training data로 준비시켜라\n정규화를 시키는 이유: 값이 이동할 때 unit을 고려: 1달러=1000원으로 같은 효과를 주기 위해서\ntree계열로 할 때는 정규화 필요가 없는데, 어떤 기준값보다 큰지 작은지만 판단하기 때문이다!(추후 수업)\n4.3 juice the all_data2 and split\n전처리한 data를 다시 빼내서 all_data2로 가져와라잇 더미코딩을 해줬기 때문에 col의 수가 늘어난다.\n\n\ncode\n\nall_data2 <- juice(housing_recipe)\nall_data2 %>% dim()\n\n\n[1] 2919  233\n\nWe are done for preprocessing. Let’s split the data set.\n전처리가 끝났으면 다시 train이랑 test로 분리시킴. 왜냐하면 train으로 학습하고 test로 예측할것이기 때문에\n\n\ncode\n\ntrain_index <- seq_len(nrow(train))\ntrain2 <- all_data2[train_index,]\ntest2 <- all_data2[-train_index,]\n\n\n\n\n\ncode\n\ntrain2 %>% \n  head() %>% \n  kable()\n\n\nms_sub_class\nlot_frontage\nlot_area\noverall_qual\noverall_cond\nyear_built\nyear_remod_add\nmas_vnr_area\nbsmt_fin_sf1\nbsmt_fin_sf2\nbsmt_unf_sf\ntotal_bsmt_sf\nx1st_flr_sf\nx2nd_flr_sf\nlow_qual_fin_sf\ngr_liv_area\nbsmt_full_bath\nbsmt_half_bath\nfull_bath\nhalf_bath\nbedroom_abv_gr\nkitchen_abv_gr\ntot_rms_abv_grd\nfireplaces\ngarage_yr_blt\ngarage_cars\ngarage_area\nwood_deck_sf\nopen_porch_sf\nenclosed_porch\nx3ssn_porch\nscreen_porch\npool_area\nmisc_val\nmo_sold\nyr_sold\nsale_price\nms_zoning_FV\nms_zoning_RH\nms_zoning_RL\nms_zoning_RM\nstreet_Pave\nlot_shape_IR2\nlot_shape_IR3\nlot_shape_Reg\nland_contour_HLS\nland_contour_Low\nland_contour_Lvl\nutilities_NoSeWa\nlot_config_CulDSac\nlot_config_FR2\nlot_config_FR3\nlot_config_Inside\nland_slope_Mod\nland_slope_Sev\nneighborhood_Blueste\nneighborhood_BrDale\nneighborhood_BrkSide\nneighborhood_ClearCr\nneighborhood_CollgCr\nneighborhood_Crawfor\nneighborhood_Edwards\nneighborhood_Gilbert\nneighborhood_IDOTRR\nneighborhood_MeadowV\nneighborhood_Mitchel\nneighborhood_NAmes\nneighborhood_NoRidge\nneighborhood_NPkVill\nneighborhood_NridgHt\nneighborhood_NWAmes\nneighborhood_OldTown\nneighborhood_Sawyer\nneighborhood_SawyerW\nneighborhood_Somerst\nneighborhood_StoneBr\nneighborhood_SWISU\nneighborhood_Timber\nneighborhood_Veenker\ncondition1_Feedr\ncondition1_Norm\ncondition1_PosA\ncondition1_PosN\ncondition1_RRAe\ncondition1_RRAn\ncondition1_RRNe\ncondition1_RRNn\ncondition2_Feedr\ncondition2_Norm\ncondition2_PosA\ncondition2_PosN\ncondition2_RRAe\ncondition2_RRAn\ncondition2_RRNn\nbldg_type_X2fmCon\nbldg_type_Duplex\nbldg_type_Twnhs\nbldg_type_TwnhsE\nhouse_style_X1.5Unf\nhouse_style_X1Story\nhouse_style_X2.5Fin\nhouse_style_X2.5Unf\nhouse_style_X2Story\nhouse_style_SFoyer\nhouse_style_SLvl\nroof_style_Gable\nroof_style_Gambrel\nroof_style_Hip\nroof_style_Mansard\nroof_style_Shed\nroof_matl_CompShg\nroof_matl_Membran\nroof_matl_Metal\nroof_matl_Roll\nroof_matl_Tar.Grv\nroof_matl_WdShake\nroof_matl_WdShngl\nexterior1st_AsphShn\nexterior1st_BrkComm\nexterior1st_BrkFace\nexterior1st_CBlock\nexterior1st_CemntBd\nexterior1st_HdBoard\nexterior1st_ImStucc\nexterior1st_MetalSd\nexterior1st_Plywood\nexterior1st_Stone\nexterior1st_Stucco\nexterior1st_VinylSd\nexterior1st_Wd.Sdng\nexterior1st_WdShing\nexterior2nd_AsphShn\nexterior2nd_Brk.Cmn\nexterior2nd_BrkFace\nexterior2nd_CBlock\nexterior2nd_CmentBd\nexterior2nd_HdBoard\nexterior2nd_ImStucc\nexterior2nd_MetalSd\nexterior2nd_Other\nexterior2nd_Plywood\nexterior2nd_Stone\nexterior2nd_Stucco\nexterior2nd_VinylSd\nexterior2nd_Wd.Sdng\nexterior2nd_Wd.Shng\nmas_vnr_type_BrkFace\nmas_vnr_type_None\nmas_vnr_type_Stone\nexter_qual_Fa\nexter_qual_Gd\nexter_qual_TA\nexter_cond_Fa\nexter_cond_Gd\nexter_cond_Po\nexter_cond_TA\nfoundation_CBlock\nfoundation_PConc\nfoundation_Slab\nfoundation_Stone\nfoundation_Wood\nbsmt_qual_Fa\nbsmt_qual_Gd\nbsmt_qual_TA\nbsmt_cond_Gd\nbsmt_cond_Po\nbsmt_cond_TA\nbsmt_exposure_Gd\nbsmt_exposure_Mn\nbsmt_exposure_No\nbsmt_fin_type1_BLQ\nbsmt_fin_type1_GLQ\nbsmt_fin_type1_LwQ\nbsmt_fin_type1_Rec\nbsmt_fin_type1_Unf\nbsmt_fin_type2_BLQ\nbsmt_fin_type2_GLQ\nbsmt_fin_type2_LwQ\nbsmt_fin_type2_Rec\nbsmt_fin_type2_Unf\nheating_GasA\nheating_GasW\nheating_Grav\nheating_OthW\nheating_Wall\nheating_qc_Fa\nheating_qc_Gd\nheating_qc_Po\nheating_qc_TA\ncentral_air_Y\nelectrical_FuseF\nelectrical_FuseP\nelectrical_Mix\nelectrical_SBrkr\nkitchen_qual_Fa\nkitchen_qual_Gd\nkitchen_qual_TA\nfunctional_Maj2\nfunctional_Min1\nfunctional_Min2\nfunctional_Mod\nfunctional_Sev\nfunctional_Typ\ngarage_type_Attchd\ngarage_type_Basment\ngarage_type_BuiltIn\ngarage_type_CarPort\ngarage_type_Detchd\ngarage_finish_RFn\ngarage_finish_Unf\ngarage_qual_Fa\ngarage_qual_Gd\ngarage_qual_Po\ngarage_qual_TA\ngarage_cond_Fa\ngarage_cond_Gd\ngarage_cond_Po\ngarage_cond_TA\npaved_drive_P\npaved_drive_Y\nsale_type_Con\nsale_type_ConLD\nsale_type_ConLI\nsale_type_ConLw\nsale_type_CWD\nsale_type_New\nsale_type_Oth\nsale_type_WD\nsale_condition_AdjLand\nsale_condition_Alloca\nsale_condition_Family\nsale_condition_Normal\nsale_condition_Partial\n0.0673199\n-0.2020329\n-0.2178414\n0.6460727\n-0.5071973\n1.0460784\n0.8966793\n0.5251119\n0.5808073\n-0.2930798\n-0.9347024\n-0.4442517\n-0.7737285\n1.2071717\n-0.1011797\n0.4134764\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n0.9866803\n-0.9241529\n1.0007573\n0.3064753\n0.3488399\n-0.7406335\n0.1999717\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n0.1576185\n12.24769\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.8734664\n0.5017845\n-0.0720317\n-0.0631737\n2.1879039\n0.1547375\n-0.3955364\n-0.5721522\n1.1779104\n-0.2930798\n-0.6297885\n0.4770294\n0.2610301\n-0.7848906\n-0.1011797\n-0.4718098\n-0.8195386\n3.8217640\n0.781232\n-0.7561915\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n-0.0849858\n0.3064753\n-0.0597822\n1.6146027\n-0.7027224\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-0.4468483\n-0.6028583\n12.10901\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n10.9810599\n4.0979294\n-2.4803837\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n0.9919814\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n2.3419622\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n2.3512352\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n1.1675169\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n3.093995\n-0.2985775\n-1.4587283\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.0673199\n-0.0612694\n0.1371734\n0.6460727\n-0.5071973\n0.9800531\n0.8488195\n0.3347702\n0.0978563\n-0.2930798\n-0.2884670\n-0.2990251\n-0.6106138\n1.2351632\n-0.1011797\n0.5636589\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n0.9203319\n0.3064753\n0.6274459\n-0.7406335\n-0.0811953\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n1.0265775\n0.1576185\n12.31717\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n3.3480662\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.3025164\n-0.4366387\n-0.0783713\n0.6460727\n-0.5071973\n-1.8590326\n-0.6826955\n-0.5721522\n-0.4948563\n-0.2930798\n-0.0472664\n-0.6711682\n-0.5061185\n0.9785744\n-0.1011797\n0.4273090\n1.0868363\n-0.2498524\n-1.027187\n-0.7561915\n0.169898\n-0.2076629\n0.3494857\n0.6235248\n0.7996938\n1.6196836\n0.7853226\n-0.7406335\n-0.1847831\n3.8743031\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n-1.3633351\n11.84940\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n5.2278523\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n2.4698379\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n5.9181952\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n-0.8406993\n1.0675387\n4.7873140\n-0.0414158\n-3.4106271\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n2.2707842\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-1.3449209\n-0.1117261\n-0.2608328\n-0.0718576\n1.6571574\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n-2.1550970\n-0.3026411\n0.0673199\n0.6894691\n0.5188142\n1.3553191\n-0.5071973\n0.9470405\n0.7530998\n1.3872480\n0.4688505\n-0.2930798\n-0.1610403\n0.2115370\n-0.0371639\n1.6713642\n-0.1011797\n1.3778060\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n1.385418\n-0.2076629\n1.6238750\n0.6235248\n0.8801192\n1.6196836\n1.6861486\n0.7768341\n0.5403318\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n2.1316468\n0.1576185\n12.42922\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n6.3323719\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.1678767\n0.7363903\n0.5004295\n-0.7724201\n-0.5071973\n0.7159521\n0.5138006\n-0.5721522\n0.6378834\n-0.2930798\n-1.1303934\n-0.5804016\n-0.9266484\n0.5353755\n-0.1011797\n-0.2742013\n1.0868363\n-0.2498524\n-1.027187\n1.2323877\n-2.261142\n-0.2076629\n-0.9249036\n-0.9241529\n0.5986302\n0.3064753\n0.0330864\n-0.4244944\n-0.2587744\n-0.3595391\n12.6010642\n-0.2858865\n-0.0631394\n1.1441161\n1.3949339\n0.9180953\n11.87060\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n4.9595195\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n24.1371155\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n\n5. Set linear regression model and fitting (모델 설정 및 학습)\nlinear regression을 할건데 그 엔진은 base에 있는 lm을 사용할 것이다.\n전처리를 시킨 train2를 이용해서 fit을 시킬거다 (fit하면 변수마다의 베타값이 나온다.)\n\n\ncode\n\nlm_model <- \n    linear_reg() %>% \n    set_engine(\"lm\")\n\nlm_form_fit <- \n    lm_model %>% \n    fit(sale_price ~ ., data = train2)\n\noptions(max.print = 10)\nprint(lm_form_fit)\n\n\nparsnip model object\n\nFit time:  90ms \n\nCall:\nstats::lm(formula = sale_price ~ ., data = data)\n\nCoefficients:\n   (Intercept)    ms_sub_class    lot_frontage        lot_area  \n     1.202e+01      -1.440e-02       6.146e-03       2.267e-02  \n  overall_qual    overall_cond      year_built  year_remod_add  \n     5.840e-02       3.983e-02       5.461e-02       1.708e-02  \n  mas_vnr_area    bsmt_fin_sf1  \n     1.801e-03       6.012e-02  \n [ reached getOption(\"max.print\") -- omitted 223 entries ]\n\n6. Prediction and submit (예측 및 평가)\n\n\ncode\n\nresult <- predict(lm_form_fit, test2)\nresult %>% head()\n\n\n# A tibble: 6 x 1\n  .pred\n  <dbl>\n1  11.7\n2  12.0\n3  12.1\n4  12.2\n5  12.2\n6  12.1\n\npredict는 lm_form_fit에 들어있는 학습한 내용을 가지고, saleprice가 없는 새로운 data인 test2를 집어넣어서 predict를 해라.\n\n\ncode\n\nsubmission <- read_csv(file.path(file_path, \"sample_submission.csv\"))\nsubmission$SalePrice <- exp(result$.pred)\nwrite.csv(submission, row.names = FALSE,\n          \"baseline_regression.csv\")\n\n\n\n얻은 결과는 submission 파일로 가서 saleprice column에 exp취해서 넣어라\n6.1 회귀분석 결과\n\n\n이전 hw에서는 변수를 선택 할 때 단순히 상관관계가 높은 상위 4개만 선택하였다.\n전처리와 많은 변수들을 사용하니 sale price 예측력이 상승하였음을 확인 할 수 있다.\n\n\n\n",
    "preview": "posts/2021-03-25-hw4/distill-preview.png",
    "last_modified": "2021-03-25T04:01:08+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-18-saleprice-prediction/",
    "title": "saleprice prediction",
    "description": "응용통계학 hw3",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\n\nContents\nlibrary\n1. 데이터 불러오기\n2. 상관계수가 높은 변수 선택\n3. Linear Model사용\n4. outlier 없애기\n5. test data에 적용해서 집 값을 예측한다.\n6. 결과 값을 실제 값과 비교\n\nlibrary\n\n\ncode\n\nlibrary(tibble)\nlibrary(tidyverse)\nlibrary(reprex)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(magrittr)\nlibrary(xlsx)\nlibrary(corrplot)\n\n\n\n1. 데이터 불러오기\n\n\ncode\n\ntrain <- read_csv(\"/cloud/project/_posts/2021-03-18-saleprice-prediction/train.csv\")\ntest <- read_csv(\"/cloud/project/_posts/2021-03-18-saleprice-prediction/test.csv\")\n\ntrain_tb <- as_tibble(train)\n\n\n\n2. 상관계수가 높은 변수 선택\n\n\ncode\n\ntrain_num <- dplyr::select_if(train, is.numeric)\nnumericVars <- which(sapply(train, is.numeric))\ntrain_numVar <- train[, numericVars]\ncor_numVar <- cor(train_numVar, use='pairwise.complete.obs')\ncor_sorted <- as.matrix(sort(cor_numVar[, 'SalePrice'], decreasing = TRUE))\nCorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x) > 0.5)))\ncor_numVar <- cor_numVar[CorHigh, CorHigh]\n\ncorrplot.mixed(cor_numVar, \n               tl.col = 'black',   \n               tl.pos = 'lt',      \n               number.cex = .7)    \n\n\n\n\nhw2에서 확인한 변수와 SalePrice와 높은 상관관계를 가진 4개 변수 선택\nOverallQual\nGrLivArea\nGarageArea\nYearBuilt\n3. Linear Model사용\n\n\ncode\n\nlm.fit = lm(SalePrice ~OverallQual + GrLivArea + GarageArea + YearBuilt, data = train_tb)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = SalePrice ~ OverallQual + GrLivArea + GarageArea + \n    YearBuilt, data = train_tb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-409387  -21263   -2265   17878  298574 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.238e+05  8.516e+04  -9.673   <2e-16 ***\nOverallQual  2.311e+04  1.162e+03  19.893   <2e-16 ***\nGrLivArea    5.570e+01  2.603e+00  21.397   <2e-16 ***\nGarageArea   5.760e+01  6.257e+00   9.206   <2e-16 ***\nYearBuilt    3.815e+02  4.476e+01   8.523   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39630 on 1455 degrees of freedom\nMultiple R-squared:  0.7518,    Adjusted R-squared:  0.7512 \nF-statistic:  1102 on 4 and 1455 DF,  p-value: < 2.2e-16\n\ncode\n\nplot(lm.fit)\n\n\n\n\n4. outlier 없애기\n\n\ncode\n\nlm.fit = lm(SalePrice ~-1 + OverallQual + GrLivArea + GarageArea + YearBuilt, data = train_tb[-c(524, 692, 899, 1183, 1299),])\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = SalePrice ~ -1 + OverallQual + GrLivArea + GarageArea + \n    YearBuilt, data = train_tb[-c(524, 692, 899, 1183, 1299), \n    ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-131449  -20964   -1456   18407  214756 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \nOverallQual 27083.585    958.555   28.25   <2e-16 ***\nGrLivArea      51.462      2.412   21.34   <2e-16 ***\nGarageArea     79.145      5.521   14.34   <2e-16 ***\nYearBuilt     -50.563      2.318  -21.82   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36480 on 1451 degrees of freedom\nMultiple R-squared:  0.9652,    Adjusted R-squared:  0.9651 \nF-statistic: 1.005e+04 on 4 and 1451 DF,  p-value: < 2.2e-16\n\ncode\n\nplot(lm.fit)\n\n\n\n\n5. test data에 적용해서 집 값을 예측한다.\n\n\ncode\n\npred <- predict(lm.fit, test, interval = \"confidence\")\nhead(pred)\n\n\n       fit      lwr      upr\n1 140150.1 135067.7 145232.4\n2 156585.7 154076.1 159095.4\n3 156423.6 153301.2 159546.0\n4 181220.3 179228.2 183212.5\n5 221866.3 217414.0 226318.6\n6 181723.3 179620.9 183825.8\n\n6. 결과 값을 실제 값과 비교\n\n\ncode\n\n# write.csv(pred, \"C:/Users/hanby/Documents/21as/new.csv\")\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-18-saleprice-prediction/saleprice-prediction_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-25T02:12:55+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-10-hw2/",
    "title": "AmesHousing",
    "description": "응용통계학 hw2",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\n\nContents\nlibrary\n1. Square Feet & Garage Area & Sale Price\n2. Heating QC & Year Built & Sale Price\n3. GasA & Kitchen Quality & Central Air Conditioning\n4. 결론\n\nlibrary\n\n\ncode\n\nlibrary(tidyverse)\nlibrary(AmesHousing)\nlibrary(ggplot2)\n\n\n\n1. Square Feet & Garage Area & Sale Price\n집 평수가 넓을수록 주차공간이 넓지 않을까?\n\n\ncode\n\nGC <- guide_legend(title = \"Garage Cars\", ncol = 7)\names_raw %>% \n  mutate(Square_Feet=`1st Flr SF`+`2nd Flr SF`) %>% \n  ggplot() + geom_point(aes(x=`Square_Feet`,y=`Garage Area`,color=as_factor(`Garage Cars`), alpha=0.3, size=`SalePrice`))+\n  scale_color_brewer(palette = \"Set1\", labels = c(\"0\", \"1\", \"2\", \"3\", \"4\",\"5\",\"NA\"))+ \n  scale_alpha_identity(0.3) + \n  guides(color = GC) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n1층과 2층 square feet를 더해서 집 평수를 계산하였다. 집 평수와 주차공간은 양의 관계를 보이며 차고에 자동차 수가 많아짐을 볼 수 있다. 또한 집 평수가 넓어질수록 집의 가격(point size)가 커짐을 확인 할 수 있다.\n2. Heating QC & Year Built & Sale Price\n난방시설이 잘 되어있을수록 집 값이 높지 않을까? 난방시설을 그룹화해서 평균 집 값을 보자!\n\nEx: Excellent\nGd: Good\nTA: Average/Typical\nFa: Fair\nPo: Poor\n\n\n\ncode\n\names_raw %>% \n  group_by(`Heating QC`) %>%\n  summarise(across(SalePrice,mean,na.rm=TRUE))\n\n\n# A tibble: 5 x 2\n  `Heating QC` SalePrice\n  <chr>            <dbl>\n1 Ex             216367.\n2 Fa             122950.\n3 Gd             156855.\n4 Po              69033.\n5 TA             138985.\n\n난방시설의 quality가 좋을수록 평균 집 가격이 높다는 것이 확인되었다.\n또한 집이 최신에 지어질 수록 난방시절이 좋을 것 같다고 생각하고 각 quality별로 지어진 가장 최근년도를 보았다.\n\n\ncode\n\names_raw %>% \n  group_by(`Heating QC`) %>%\n  summarise(across(`Year Built`,max,na.rm=TRUE))\n\n\n# A tibble: 5 x 2\n  `Heating QC` `Year Built`\n  <chr>               <int>\n1 Ex                   2010\n2 Fa                   1978\n3 Gd                   2009\n4 Po                   1952\n5 TA                   1992\n\n확인결과 최근에 지어질수록 난방시설이 좋아지고 집 가격이 올라갈 것이라고 생각하고, ggplot 을 이용하여 연도별 sale price에 대한 난방시설 그래프를 그려보았다.\n\n\ncode\n\nhqc <- guide_legend(title = \"Heating Quality and Condition\", nrow = 5)\names_raw %>% \n  group_by(`Heating QC`) %>%\n  ggplot(aes(x=`Year Built`,y=SalePrice)) + \n  geom_point(aes(color=as_factor(`Heating QC`),alpha=0.3))+\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Fair\", \"Typical\", \"Excellent\", \"Good\", \"Poor\"))+ \n  scale_alpha_identity(0.3) + \n  guides(color = hqc) +\n  theme(legend.position = \"right\")\n\n\n\n\n위 그래프들을 난방 종류에 따라 나누어본다.\n\n\ncode\n\nhqc <- guide_legend(title = \"Heating Quality and Condition\", nrow = 5)\names_raw %>% \n  group_by(`Heating QC`) %>%\n  ggplot(aes(x=`Year Built`,y=SalePrice)) + \n  geom_point(aes(color=as_factor(`Heating QC`),alpha=0.3))+\n  scale_color_brewer(palette = \"Set1\",\n                     labels = c(\"Fair\", \"Typical\", \"Excellent\", \"Good\", \"Poor\"))+ \n  scale_alpha_identity(0.3) + \n  guides(color = hqc) +\n  theme(legend.position = \"right\") + facet_wrap(~`Heating`)\n\n\n\n\n난방 종류별로 나누어본 결과 난방시설이 Excellent(초록색점)인 것은 모두 GasA(Gas forced warm air furnace를 사용하고 있는 것을 알 수 있다. 또한 최근으로 갈수록 GasA Type의 집을 많이 짓는다는 것을 확인 할 수있다(색이 짙어짐). 그럼 AmesHousing dataset을 GasA인 것만 필터링하여 살펴 보자.\n3. GasA & Kitchen Quality & Central Air Conditioning\nGasA로 난방을 사용하는 집안의 다른 시설 보기.\n\n\ncode\n\names_raw %>% \n         filter(Heating==\"GasA\") %>% \n         ggplot(aes(x=`Kitchen Qual`,y=SalePrice,color=`Kitchen Qual`)) + \n  geom_boxplot()+\n  facet_wrap(~ `Central Air`)\n\n\n\n\n같은 난방시설(GasA)을 사용하는 주방시설의 Quality도 중앙 에어컨(? 중앙제어 난방이라고 이해했음..)이 아닌 개별로 에어컨을 사용할 수 있는 것이 같은 Kitchen Quality에서도 집 값이 높다는 것을 확인 할 수 있다.\n4. 결론\n집 평수가 넓어질수록 주차를 할 수 있는 공간이 넓어지며 차고의 자동차 갯수가 증가하고, 집 가격도 상승한다.\nHeating Quality가 좋은(Excellent)집은 모두 GasA type이며 최근으로 갈수록 GasA Type의 집을 많이 짓고 있다.\n같은 난방시설과 난방 유형을 가진 집에서도 중앙에어컨인 경우보다 개별적으로 조절할 수 있는 집의 가격이 더 높다.\n\n\n\n",
    "preview": "posts/2021-03-10-hw2/hw2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-25T02:13:34+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-07-lec2/",
    "title": "ggplot2",
    "description": "응용통계학 lec2",
    "author": [
      {
        "name": "hanbyeol",
        "url": {}
      }
    ],
    "date": "2021-03-07",
    "categories": [],
    "contents": "\n\nContents\n6가지 중요한 dplyr\n1. filter(): 특정 조건을 만족하는 것을 필터링\nQ.Adelie이거나 Gentoo이면서 bill lenth가 30에서 100 사이인 것은?\n\n2. select(): column을 기준으로하는 operator\n3. mutate(): 새로운 변수를 만들어 열을 추가 할 때\n4. arrange(): data를 정렬하는 함수, 기본은 오름차순\n5. summarize(): 기존의 값으로 새로운 변수를 만들어서 요약해서 보여줌\n6. across(): 여러개의 column에 같은 함수를 적용할 때(cf.col별로 새로운 변수를 만들었음)\n\n\n\n\ncode\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n\n\n6가지 중요한 dplyr\n1. filter(): 특정 조건을 만족하는 것을 필터링\n\n\ncode\n\n    penguins %>%\n        filter(species==\"Chinstrap\") %>% \n  head()\n\n\n# A tibble: 6 x 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>     <fct>           <dbl>         <dbl>             <int>\n1 Chinstrap Dream            46.5          17.9               192\n2 Chinstrap Dream            50            19.5               196\n3 Chinstrap Dream            51.3          19.2               193\n4 Chinstrap Dream            45.4          18.7               188\n5 Chinstrap Dream            52.7          19.8               197\n6 Chinstrap Dream            45.2          17.8               198\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\nand 조건\n\n\ncode\n\n    penguins %>%\n        filter(species==\"Chinstrap\",island==\"Dream\") %>% \n  head()\n\n\n# A tibble: 6 x 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>     <fct>           <dbl>         <dbl>             <int>\n1 Chinstrap Dream            46.5          17.9               192\n2 Chinstrap Dream            50            19.5               196\n3 Chinstrap Dream            51.3          19.2               193\n4 Chinstrap Dream            45.4          18.7               188\n5 Chinstrap Dream            52.7          19.8               197\n6 Chinstrap Dream            45.2          17.8               198\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\nlogical operator쓰면\n\n\ncode\n\n    penguins %>% \n        filter((species==\"Chinstrap\"|species==\"Adelie\")&island==\"dream\") %>% \n  head()\n\n\n# A tibble: 0 x 8\n# … with 8 variables: species <fct>, island <fct>,\n#   bill_length_mm <dbl>, bill_depth_mm <dbl>,\n#   flipper_length_mm <int>, body_mass_g <int>, sex <fct>, year <int>\n\nor조건\n\n\ncode\n\n    penguins %>%\n        filter(species %in% c(\"Chinstrap\",\"Adelie\"))\n\n\n# A tibble: 220 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 210 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\ncode\n\n    penguins %>%\n        filter(species %in% c(\"Chinstrap\",\"Adelie\"),island==\"dream\") %>% \n      head()\n\n\n# A tibble: 0 x 8\n# … with 8 variables: species <fct>, island <fct>,\n#   bill_length_mm <dbl>, bill_depth_mm <dbl>,\n#   flipper_length_mm <int>, body_mass_g <int>, sex <fct>, year <int>\n\nQ.Adelie이거나 Gentoo이면서 bill lenth가 30에서 100 사이인 것은?\nA.filter사용시\n\n\ncode\n\n    penguins %>% \n        filter(species %in% c(\"Adelie\",\"Gentoo\"),\n               bill_length_mm>30 & bill_length_mm<100) %>% \n        nrow()\n\n\n[1] 274\n\n2. select(): column을 기준으로하는 operator\n\n\ncode\n\n    penguins %>% \n        select(species,bill_length_mm,bill_depth_mm) %>% \n        head()\n\n\n# A tibble: 6 x 3\n  species bill_length_mm bill_depth_mm\n  <fct>            <dbl>         <dbl>\n1 Adelie            39.1          18.7\n2 Adelie            39.5          17.4\n3 Adelie            40.3          18  \n4 Adelie            NA            NA  \n5 Adelie            36.7          19.3\n6 Adelie            39.3          20.6\n\n해당 열(species)을 제외하고 선택해줘\n\n\ncode\n\n    penguins %>% \n        select(-species) %>% \n        head()\n\n\n# A tibble: 6 x 7\n  island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>              <dbl>         <dbl>             <int>       <int>\n1 Torgersen           39.1          18.7               181        3750\n2 Torgersen           39.5          17.4               186        3800\n3 Torgersen           40.3          18                 195        3250\n4 Torgersen           NA            NA                  NA          NA\n5 Torgersen           36.7          19.3               193        3450\n6 Torgersen           39.3          20.6               190        3650\n# … with 2 more variables: sex <fct>, year <int>\n\n여러개의 연속적인 열을 선택해줘\n\n\ncode\n\n    penguins %>% \n        select(bill_length_mm:body_mass_g) %>% \n  head()\n\n\n# A tibble: 6 x 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           <dbl>         <dbl>             <int>       <int>\n1           39.1          18.7               181        3750\n2           39.5          17.4               186        3800\n3           40.3          18                 195        3250\n4           NA            NA                  NA          NA\n5           36.7          19.3               193        3450\n6           39.3          20.6               190        3650\n\n특정조건을 만족시키는 열을 선택해줘\n\n\ncode\n\n    penguins %>% \n        select(ends_with(\"mm\")) %>% \n        names()\n\n\n[1] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n\n내가 선택한 순서대로 선택해줘(선택한것빼고 나머지st)\n\n\ncode\n\n    penguins %>% \n        select(island,bill_length_mm,everything()) %>% \n        head()\n\n\n# A tibble: 6 x 8\n  island    bill_length_mm species bill_depth_mm flipper_length_mm\n  <fct>              <dbl> <fct>           <dbl>             <int>\n1 Torgersen           39.1 Adelie           18.7               181\n2 Torgersen           39.5 Adelie           17.4               186\n3 Torgersen           40.3 Adelie           18                 195\n4 Torgersen           NA   Adelie           NA                  NA\n5 Torgersen           36.7 Adelie           19.3               193\n6 Torgersen           39.3 Adelie           20.6               190\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\n3. mutate(): 새로운 변수를 만들어 열을 추가 할 때\n\n\ncode\n\n   penguins %>% \n        select(species,bill_length_mm,bill_depth_mm) %>% \n        mutate(bill_total=bill_length_mm + bill_depth_mm) %>% \n        head()\n\n\n# A tibble: 6 x 4\n  species bill_length_mm bill_depth_mm bill_total\n  <fct>            <dbl>         <dbl>      <dbl>\n1 Adelie            39.1          18.7       57.8\n2 Adelie            39.5          17.4       56.9\n3 Adelie            40.3          18         58.3\n4 Adelie            NA            NA         NA  \n5 Adelie            36.7          19.3       56  \n6 Adelie            39.3          20.6       59.9\n\n새로 만든 변수를 바로 쓸수도 있다!\n\n\ncode\n\n    penguins %>% \n        select(species,bill_length_mm,bill_depth_mm) %>% \n        mutate(bill_total=bill_length_mm + bill_depth_mm,\n               bill_average=bill_total/2) %>% \n        head()\n\n\n# A tibble: 6 x 5\n  species bill_length_mm bill_depth_mm bill_total bill_average\n  <fct>            <dbl>         <dbl>      <dbl>        <dbl>\n1 Adelie            39.1          18.7       57.8         28.9\n2 Adelie            39.5          17.4       56.9         28.4\n3 Adelie            40.3          18         58.3         29.2\n4 Adelie            NA            NA         NA           NA  \n5 Adelie            36.7          19.3       56           28  \n6 Adelie            39.3          20.6       59.9         30.0\n\ntransmutate(): 새롭게 만든 변수만 선택하고 나머지는 버림\n\n\ncode\n\n    penguins %>% \n        select(species,bill_length_mm,bill_depth_mm) %>% \n        transmute(bill_total=bill_length_mm + bill_depth_mm,\n                  bill_average=bill_total/2) %>% \n        head()\n\n\n# A tibble: 6 x 2\n  bill_total bill_average\n       <dbl>        <dbl>\n1       57.8         28.9\n2       56.9         28.4\n3       58.3         29.2\n4       NA           NA  \n5       56           28  \n6       59.9         30.0\n\n4. arrange(): data를 정렬하는 함수, 기본은 오름차순\n\n\ncode\n\npenguins %>%\n\nselect(species,bill_length_mm,bill_depth_mm) %>%\n\nmutate(bill_length_mm=ceiling(bill_length_mm),\n\nbill_depth_mm=ceiling(bill_depth_mm)) %>%\n\narrange(bill_length_mm)\n\n\n# A tibble: 344 x 3\n   species bill_length_mm bill_depth_mm\n   <fct>            <dbl>         <dbl>\n 1 Adelie              33            16\n 2 Adelie              34            19\n 3 Adelie              34            18\n 4 Adelie              34            17\n 5 Adelie              35            19\n 6 Adelie              35            22\n 7 Adelie              35            19\n 8 Adelie              35            18\n 9 Adelie              35            19\n10 Adelie              35            18\n# … with 334 more rows\n\nbill_length_mm이 겹치면 bill_depth_mm으로 정렬하라\n\n\ncode\n\npenguins %>%\n\nselect(species,bill_length_mm,bill_depth_mm) %>%\n\nmutate(bill_length_mm=ceiling(bill_length_mm),\n\nbill_depth_mm=ceiling(bill_depth_mm)) %>%\n\narrange(bill_length_mm,bill_depth_mm)\n\n\n# A tibble: 344 x 3\n   species bill_length_mm bill_depth_mm\n   <fct>            <dbl>         <dbl>\n 1 Adelie              33            16\n 2 Adelie              34            17\n 3 Adelie              34            18\n 4 Adelie              34            19\n 5 Adelie              35            18\n 6 Adelie              35            18\n 7 Adelie              35            18\n 8 Adelie              35            19\n 9 Adelie              35            19\n10 Adelie              35            19\n# … with 334 more rows\n\nbill_length_mm이 겹치면 bill_depth_mm으로 “내림차순”으로 정렬하라\n\n\ncode\n\npenguins %>%\n  select(species,bill_length_mm,bill_depth_mm) %>%\n  mutate(bill_length_mm=ceiling(bill_length_mm),bill_depth_mm=ceiling(bill_depth_mm)) %>%\n\n  arrange(bill_length_mm, desc(bill_depth_mm))\n\n\n# A tibble: 344 x 3\n   species bill_length_mm bill_depth_mm\n   <fct>            <dbl>         <dbl>\n 1 Adelie              33            16\n 2 Adelie              34            19\n 3 Adelie              34            18\n 4 Adelie              34            17\n 5 Adelie              35            22\n 6 Adelie              35            19\n 7 Adelie              35            19\n 8 Adelie              35            19\n 9 Adelie              35            18\n10 Adelie              35            18\n# … with 334 more rows\n\n5. summarize(): 기존의 값으로 새로운 변수를 만들어서 요약해서 보여줌\n\n\ncode\n\npenguins %>%\n\nsummarise(bill_length_mean = mean(bill_length_mm, na.rm = TRUE),\n\nbill_depth_mean = mean(bill_depth_mm, na.rm = TRUE))\n\n\n# A tibble: 1 x 2\n  bill_length_mean bill_depth_mean\n             <dbl>           <dbl>\n1             43.9            17.2\n\n그룹을 만들어서 새로운 변수를 만들고 요약해서 보여줌\n\n\ncode\n\npenguins %>%\n\ngroup_by(species) %>%\n\nsummarise(bill_depth_mean=mean(bill_depth_mm,na.rm=TRUE),\n\nbill_length_mean=mean(bill_length_mm,na.rm=TRUE))\n\n\n# A tibble: 3 x 3\n  species   bill_depth_mean bill_length_mean\n  <fct>               <dbl>            <dbl>\n1 Adelie               18.3             38.8\n2 Chinstrap            18.4             48.8\n3 Gentoo               15.0             47.5\n\n6. across(): 여러개의 column에 같은 함수를 적용할 때(cf.col별로 새로운 변수를 만들었음)\nacross가 없을 때\n\n\ncode\n\npenguins %>%\n\ngroup_by(species) %>%\n\nsummarise(bill_depth_mean=mean(bill_depth_mm,na.rm=TRUE),\n\nbill_length_mean=mean(bill_length_mm,na.rm=TRUE),\n\nflipper_length_mean=mean(flipper_length_mm,na.rm=TRUE))\n\n\n# A tibble: 3 x 4\n  species   bill_depth_mean bill_length_mean flipper_length_mean\n  <fct>               <dbl>            <dbl>               <dbl>\n1 Adelie               18.3             38.8                190.\n2 Chinstrap            18.4             48.8                196.\n3 Gentoo               15.0             47.5                217.\n\nacross를 사용하면\n\n\ncode\n\npenguins %>%\n\ngroup_by(species) %>%\n\nsummarise(across(bill_length_mm:flipper_length_mm,mean,na.rm=TRUE))\n\n\n# A tibble: 3 x 4\n  species   bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>              <dbl>         <dbl>             <dbl>\n1 Adelie              38.8          18.3              190.\n2 Chinstrap           48.8          18.4              196.\n3 Gentoo              47.5          15.0              217.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-25T02:14:32+00:00",
    "input_file": {}
  }
]
